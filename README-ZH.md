# æ·±åº¦å­¦ä¹ è°ƒä¼˜æ‰‹å†Œ

_This is not an officially supported Google product._

**Varun Godbole<sup>&dagger;</sup>, George E. Dahl<sup>&dagger;</sup>, Justin Gilmer<sup>&dagger;</sup>, Christopher J. Shallue<sup>&Dagger;</sup>, Zachary Nado<sup>&dagger;</sup>**

&dagger; Google Research, Brain Team

&Dagger; Harvard University

## Table of Contents

- [Who is this document for? è¿™ä»½æ–‡æ¡£é€‚ç”¨äºè°ï¼Ÿ](#who-is-this-document-for)
- [Why a tuning playbook? ä¸ºä»€ä¹ˆéœ€è¦ä¸€ä¸ªè°ƒä¼˜æ‰‹å†Œï¼Ÿ](#why-a-tuning-playbook)
- [Guide for starting a new project å¯åŠ¨æ–°é¡¹ç›®æŒ‡å—](#guide-for-starting-a-new-project)
  - [Choosing the model architecture é€‰æ‹©æ¨¡å‹æ¶æ„](#choosing-the-model-architecture)
  - [Choosing the optimizer é€‰æ‹©ä¼˜åŒ–å™¨](#choosing-the-optimizer)
  - [Choosing the batch size é€‰æ‹©æ‰¹é‡å¤§å°](#choosing-the-batch-size)
  - [Choosing the initial configuration é€‰æ‹©åˆå§‹é…ç½®](#choosing-the-initial-configuration)
- [A scientific approach to improving model performance æå‡æ¨¡å‹æ€§èƒ½çš„ç§‘å­¦æ–¹æ³•](#a-scientific-approach-to-improving-model-performance)
  - [The incremental tuning strategy æ¸è¿›è°ƒæ•´ç­–ç•¥](#the-incremental-tuning-strategy)
  - [Exploration vs exploitation æ¢ç´¢ä¸å¼€å‘](#exploration-vs-exploitation)
  - [Choosing the goal for the next round of experiments é€‰æ‹©ä¸‹ä¸€è½®å®éªŒçš„ç›®æ ‡](#choosing-the-goal-for-the-next-round-of-experiments)
- [Designing the next round of experiments è®¾è®¡ä¸‹ä¸€è½®å®éªŒ](#designing-the-next-round-of-experiments)
  - [Determining whether to adopt a training pipeline change or
    hyperparameter
    configuration](#determining-whether-to-adopt-a-training-pipeline-change-or-hyperparameter-configuration)
  - [After exploration concludes](#after-exploration-concludes)
- [Determining the number of steps for each training run](#determining-the-number-of-steps-for-each-training-run)
  - [Deciding how long to train when training is not compute-bound](#deciding-how-long-to-train-when-training-is-not-compute-bound)
  - [Deciding how long to train when training is compute-bound](#deciding-how-long-to-train-when-training-is-compute-bound)
- [Additional guidance for the training pipeline](#additional-guidance-for-the-training-pipeline)
  - [Optimizing the input pipeline](#optimizing-the-input-pipeline)
  - [Evaluating model performance](#evaluating-model-performance)
  - [Saving checkpoints and retrospectively selecting the best checkpoint](#saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint)
  - [Setting up experiment tracking](#setting-up-experiment-tracking)
  - [Batch normalization implementation details](#batch-normalization-implementation-details)
  - [Considerations for multi-host pipelines](#considerations-for-multi-host-pipelines)
- [FAQs](#faqs)
- [Acknowledgments](#acknowledgments)
- [Citing](#citing)
- [Contributing](#contributing)

## Who is this document for?

**è¿™ä»½æ–‡æ¡£é€‚ç”¨äºè°ï¼Ÿ**

æœ¬æ–‡æ—¨åœ¨ä¸ºå¯¹**æœ€å¤§åŒ–æ·±åº¦å­¦ä¹ æ¨¡å‹**æ€§èƒ½æ„Ÿå…´è¶£çš„å·¥ç¨‹å¸ˆå’Œç ”ç©¶äººå‘˜ï¼ˆæ— è®ºæ˜¯ä¸ªäººè¿˜æ˜¯å›¢é˜Ÿï¼‰æä¾›å¸®åŠ©ã€‚æˆ‘ä»¬å‡è®¾è¯»è€…å…·å¤‡æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¦‚å¿µçš„åŸºæœ¬çŸ¥è¯†ã€‚

æˆ‘ä»¬é‡ç‚¹å…³æ³¨**è¶…å‚æ•°è°ƒæ•´çš„è¿‡ç¨‹**ï¼Œè™½ç„¶æˆ‘ä»¬æ¶‰åŠæ·±åº¦å­¦ä¹ è®­ç»ƒçš„å…¶ä»–æ–¹é¢ï¼Œå¦‚æµæ°´çº¿å®æ–½å’Œä¼˜åŒ–ï¼Œä½†æˆ‘ä»¬å¹¶æœªæ‰“ç®—è¯¦å°½è®¨è®ºè¿™äº›æ–¹é¢ã€‚

æˆ‘ä»¬å‡è®¾æœºå™¨å­¦ä¹ é—®é¢˜æ˜¯ä¸€ä¸ªç›‘ç£å­¦ä¹ é—®é¢˜ï¼Œæˆ–è€…ç±»ä¼¼äºç›‘ç£å­¦ä¹ çš„é—®é¢˜ï¼ˆä¾‹å¦‚è‡ªç›‘ç£å­¦ä¹ ï¼‰ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæœ¬æ–‡æ¡£ä¸­çš„ä¸€äº›å»ºè®®ä¹Ÿå¯èƒ½é€‚ç”¨äºå…¶ä»–ç±»å‹çš„é—®é¢˜ã€‚

## Why a tuning playbook?

**ä¸ºä»€ä¹ˆéœ€è¦ä¸€ä¸ªè°ƒä¼˜æ‰‹å†Œï¼Ÿ**

ç›®å‰ï¼Œåœ¨å®é™…å°†æ·±åº¦ç¥ç»ç½‘ç»œæœ‰æ•ˆåœ°è¿ä½œä¸­ï¼Œæ¶‰åŠåˆ°äº†ä»¤äººæƒŠè®¶çš„ç¹çå’ŒçŒœæµ‹ã€‚æ›´ç³Ÿç³•çš„æ˜¯ï¼Œäººä»¬ç”¨äºåœ¨æ·±åº¦å­¦ä¹ ä¸­è·å¾—è‰¯å¥½ç»“æœçš„å®é™…æ–¹æ³•å¾ˆå°‘æœ‰è®°å½•ã€‚è®ºæ–‡é€šå¸¸ä¼šå¿½ç•¥å¯¼è‡´æœ€ç»ˆç»“æœçš„è¿‡ç¨‹ï¼Œä»¥å‘ˆç°ä¸€ä¸ªæ›´å¹²å‡€çš„æ•…äº‹ï¼Œè€Œåœ¨å¤„ç†å•†ä¸šé—®é¢˜çš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆå¾ˆå°‘æœ‰æ—¶é—´é€€åä¸€æ­¥ï¼Œæ€»ç»“å½’çº³ä»–ä»¬çš„è¿‡ç¨‹ã€‚
æ•™ç§‘ä¹¦å¾€å¾€å›é¿å®ç”¨æŒ‡å¯¼ï¼Œä¼˜å…ˆè€ƒè™‘åŸºæœ¬åŸç†ï¼Œå³ä½¿å®ƒä»¬çš„ä½œè€…åœ¨åº”ç”¨å·¥ä½œä¸­å…·å¤‡æä¾›æœ‰ç”¨å»ºè®®æ‰€éœ€çš„ç»éªŒã€‚åœ¨å‡†å¤‡åˆ›å»ºè¿™ä»½æ–‡æ¡£æ—¶ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…¨é¢çš„å°è¯•æ¥çœŸæ­£è§£é‡Šâ€œå¦‚ä½•åœ¨æ·±åº¦å­¦ä¹ ä¸­å–å¾—å¥½çš„ç»“æœâ€ã€‚ç›¸åï¼Œæˆ‘ä»¬åœ¨åšå®¢æ–‡ç« å’Œç¤¾äº¤åª’ä½“ä¸Šæ‰¾åˆ°äº†ä¸€äº›å»ºè®®ç‰‡æ®µï¼Œä»ç ”ç©¶è®ºæ–‡çš„é™„å½•ä¸­æ‰¾åˆ°äº†ä¸€äº›æŠ€å·§ï¼Œå¶å°”æœ‰å…³äºæŸä¸ªç‰¹å®šé¡¹ç›®æˆ–æµç¨‹çš„æ¡ˆä¾‹ç ”ç©¶ï¼Œä»¥åŠè®¸å¤šæ··ä¹±çš„ä¿¡æ¯ã€‚æ·±åº¦å­¦ä¹ ä¸“å®¶å’Œä½¿ç”¨è¡¨é¢ä¸Šç›¸ä¼¼æ–¹æ³•çš„æŠ€èƒ½è¾ƒä½çš„ä»ä¸šè€…ä¹‹é—´å­˜åœ¨å·¨å¤§çš„å·®è·ã€‚
åŒæ—¶ï¼Œè¿™äº›ä¸“å®¶ä¹Ÿå¦æ‰¿ï¼Œä»–ä»¬æ‰€åšçš„ä¸€äº›äº‹æƒ…å¯èƒ½å¹¶æ²¡æœ‰å¾ˆå¥½çš„ç†ç”±ã€‚éšç€æ·±åº¦å­¦ä¹ çš„æˆç†Ÿå’Œåœ¨ä¸–ç•Œä¸Šäº§ç”Ÿæ›´å¤§å½±å“ï¼Œç¤¾åŒºéœ€è¦æ›´å¤šè¦†ç›–æœ‰ç”¨æ–¹æ³•çš„èµ„æºï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯¹è·å–è‰¯å¥½ç»“æœè‡³å…³é‡è¦çš„å®é™…ç»†èŠ‚ã€‚

æˆ‘ä»¬æ˜¯ä¸€ä¸ªç”±äº”åç ”ç©¶äººå‘˜å’Œå·¥ç¨‹å¸ˆç»„æˆçš„å›¢é˜Ÿï¼Œå¤šå¹´æ¥ä¸€ç›´ä»äº‹æ·±åº¦å­¦ä¹ å·¥ä½œï¼Œå…¶ä¸­ä¸€äº›äººç”šè‡³ä» 2006 å¹´å°±å¼€å§‹ã€‚æˆ‘ä»¬å°†æ·±åº¦å­¦ä¹ åº”ç”¨äºä»è¯­éŸ³è¯†åˆ«åˆ°å¤©æ–‡å­¦çš„å„ç§é—®é¢˜ï¼Œå¹¶åœ¨è¿™ä¸€è¿‡ç¨‹ä¸­ç§¯ç´¯äº†ä¸°å¯Œçš„ç»éªŒã€‚
è¿™ä»½æ–‡æ¡£æºäºæˆ‘ä»¬è‡ªå·±åœ¨è®­ç»ƒç¥ç»ç½‘ç»œã€æŒ‡å¯¼æ–°çš„æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆä»¥åŠå°±æ·±åº¦å­¦ä¹ å®è·µå‘åŒäº‹æä¾›å»ºè®®çš„ç»éªŒã€‚å°½ç®¡çœ‹åˆ°æ·±åº¦å­¦ä¹ ä»ä¸€ç§ç”±å°‘æ•°å­¦æœ¯å®éªŒå®¤å®è·µçš„æœºå™¨å­¦ä¹ æ–¹æ³•ï¼Œå‘å±•æˆä¸ºæ”¯æŒæ•°åäº¿äººä½¿ç”¨çš„äº§å“çš„æŠ€æœ¯ï¼Œè¿™è®©æˆ‘ä»¬æ„Ÿåˆ°æ»¡è¶³ï¼Œä½†æ·±åº¦å­¦ä¹ ä½œä¸ºä¸€é—¨å·¥ç¨‹å­¦ç§‘ä»å¤„äºåˆçº§é˜¶æ®µã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä»½æ–‡æ¡£èƒ½å¤Ÿé¼“åŠ±å…¶ä»–äººå¸®åŠ©ç³»ç»ŸåŒ–è¯¥é¢†åŸŸçš„å®éªŒæ€§åè®®ã€‚

è¿™ä»½æ–‡æ¡£æ˜¯æˆ‘ä»¬è¯•å›¾æ¢³ç†è‡ªå·±å¯¹æ·±åº¦å­¦ä¹ æ–¹æ³•çš„ç†è§£æ—¶äº§ç”Ÿçš„ï¼Œå› æ­¤å®ƒä»£è¡¨äº†ä½œè€…åœ¨æ’°å†™æ—¶çš„è§‚ç‚¹ï¼Œè€Œä¸æ˜¯ä»»ä½•å®¢è§‚çœŸç†ã€‚æˆ‘ä»¬åœ¨è¶…å‚æ•°è°ƒæ•´æ–¹é¢çš„å›°æ‰°ä½¿å…¶æˆä¸ºæˆ‘ä»¬æŒ‡å¯¼çš„ä¸€ä¸ªç‰¹åˆ«å…³æ³¨ç‚¹ï¼Œä½†æˆ‘ä»¬ä¹Ÿæ¶µç›–äº†åœ¨å·¥ä½œä¸­é‡åˆ°çš„å…¶ä»–é‡è¦é—®é¢˜ï¼ˆæˆ–è€…çœ‹åˆ°è¿‡å‡ºç°é—®é¢˜çš„æƒ…å†µï¼‰ã€‚æˆ‘ä»¬çš„æ„å›¾æ˜¯è®©è¿™ä»½å·¥ä½œæˆä¸ºä¸€ä¸ªä¸æ–­å‘å±•å’Œæ¼”å˜çš„æ´»æ€æ–‡æ¡£ï¼Œä»¥åæ˜ æˆ‘ä»¬ä¿¡ä»°çš„å˜åŒ–ã€‚ä¾‹å¦‚ï¼Œæœ‰å…³è°ƒè¯•å’Œç¼“è§£è®­ç»ƒå¤±è´¥çš„ææ–™åœ¨ä¸¤å¹´å‰å¯¹æˆ‘ä»¬æ¥è¯´æ˜¯ä¸å¯èƒ½å†™å‡ºçš„ï¼Œå› ä¸ºå®ƒæ˜¯åŸºäºæœ€è¿‘çš„ç ”ç©¶å’Œæ­£åœ¨è¿›è¡Œçš„è°ƒæŸ¥ã€‚
ä¸å¯é¿å…åœ°ï¼Œæˆ‘ä»¬çš„ä¸€äº›å»ºè®®éœ€è¦æ ¹æ®æ–°çš„ç»“æœå’Œæ”¹è¿›çš„å·¥ä½œæµç¨‹è¿›è¡Œæ›´æ–°ã€‚æˆ‘ä»¬å¹¶ä¸çŸ¥é“æ·±åº¦å­¦ä¹ çš„â€œæœ€ä½³â€é…æ–¹ï¼Œä½†åœ¨ç¤¾åŒºå¼€å§‹è®°å½•å’Œè®¨è®ºä¸åŒçš„ç¨‹åºä¹‹å‰ï¼Œæˆ‘ä»¬æ— æ³•æŒ‡æœ›æ‰¾åˆ°å®ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¼“åŠ±è¯»è€…å¦‚æœå¯¹æˆ‘ä»¬çš„å»ºè®®æœ‰å¼‚è®®ï¼Œå¯ä»¥æå‡ºæ›¿ä»£æ€§çš„å»ºè®®ï¼Œå¹¶é™„ä¸Šæœ‰è¯´æœåŠ›çš„è¯æ®ï¼Œä»¥ä¾¿æˆ‘ä»¬æ›´æ–°è¿™ä»½æŒ‡å—ã€‚æˆ‘ä»¬ä¹Ÿå¸Œæœ›çœ‹åˆ°å…¶ä»–æŒ‡å—å’Œæ‰‹å†Œï¼Œå®ƒä»¬å¯èƒ½æä¾›ä¸åŒçš„å»ºè®®ï¼Œä»¥ä¾¿æˆ‘ä»¬ä½œä¸ºä¸€ä¸ªç¤¾åŒºæœç€æœ€ä½³å®è·µåŠªåŠ›ã€‚æœ€åï¼Œä»»ä½•æ ‡æœ‰ ğŸ¤– emoji çš„éƒ¨åˆ†éƒ½æ˜¯æˆ‘ä»¬å¸Œæœ›è¿›è¡Œæ›´å¤šç ”ç©¶çš„åœ°æ–¹ã€‚
åªæœ‰åœ¨å°è¯•ç¼–å†™è¿™ä»½æ‰‹å†Œä¹‹åï¼Œæ‰å˜å¾—å®Œå…¨æ¸…æ™°ï¼Œæ·±åº¦å­¦ä¹ å®è·µè€…çš„å·¥ä½œæµç¨‹ä¸­æœ‰å¤šå°‘æœ‰è¶£ä¸”è¢«å¿½è§†çš„ç ”ç©¶é—®é¢˜ã€‚

## Guide for starting a new project

**_å¯åŠ¨æ–°é¡¹ç›®æŒ‡å—_**

åœ¨è°ƒä¼˜è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åšå‡ºçš„è®¸å¤šå†³ç­–å¯ä»¥åœ¨é¡¹ç›®å¼€å§‹æ—¶è¿›è¡Œä¸€æ¬¡æ€§åˆ¶å®šï¼Œåªæœ‰åœ¨æƒ…å†µå‘ç”Ÿå˜åŒ–æ—¶å¶å°”è¿›è¡Œé‡æ–°å®¡è§†ã€‚

ä»¥ä¸‹æ˜¯æˆ‘ä»¬çš„æŒ‡å¯¼ï¼ŒåŸºäºä»¥ä¸‹å‡è®¾

- é—®é¢˜é˜è¿°ã€æ•°æ®æ¸…ç†ç­‰åŸºæœ¬å·¥ä½œå·²ç»åšå¾—è¶³å¤Ÿå……åˆ†ï¼Œå› æ­¤èŠ±æ—¶é—´åœ¨æ¨¡å‹æ¶æ„å’Œè®­ç»ƒé…ç½®ä¸Šæ˜¯æœ‰æ„ä¹‰çš„ã€‚
- å·²ç»å»ºç«‹äº†ä¸€ä¸ªå¯ä»¥è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°çš„æµç¨‹ï¼Œå¯¹äºæ„Ÿå…´è¶£çš„å„ç§æ¨¡å‹ï¼Œè½»æ¾æ‰§è¡Œè®­ç»ƒå’Œé¢„æµ‹ä»»åŠ¡ã€‚
- å·²ç»é€‰æ‹©å¹¶å®æ–½äº†é€‚å½“çš„åº¦é‡æ ‡å‡†ã€‚è¿™äº›åº¦é‡æ ‡å‡†åº”è¯¥å°½å¯èƒ½åœ°ä»£è¡¨åœ¨å®é™…éƒ¨ç½²ç¯å¢ƒä¸­è¿›è¡Œæµ‹é‡çš„å†…å®¹ã€‚

### Choosing the model architecture

**_é€‰æ‹©æ¨¡å‹æ¶æ„_**

**_æ¦‚è¦ï¼š_** _åœ¨å¼€å§‹æ–°é¡¹ç›®æ—¶ï¼Œå°½é‡é‡ç”¨å·²ç»æœ‰æ•ˆçš„æ¨¡å‹ã€‚_

- é¦–å…ˆé€‰æ‹©ä¸€ä¸ªè¢«å¹¿æ³›è®¤å¯å’Œå¸¸ç”¨çš„æ¨¡å‹æ¶æ„ï¼Œä»¥ç¡®ä¿è¿…é€Ÿè·å¾—å¯å·¥ä½œçš„æ¨¡å‹ã€‚éšåå§‹ç»ˆæœ‰å¯èƒ½æ„å»ºå®šåˆ¶æ¨¡å‹ã€‚
- æ¨¡å‹æ¶æ„é€šå¸¸å…·æœ‰å„ç§è¶…å‚æ•°ï¼Œè¿™äº›å‚æ•°å†³å®šäº†æ¨¡å‹çš„å¤§å°å’Œå…¶ä»–ç»†èŠ‚ï¼ˆä¾‹å¦‚å±‚æ•°ã€å±‚å®½åº¦ã€æ¿€æ´»å‡½æ•°ç±»å‹ç­‰ï¼‰ã€‚
  - å› æ­¤ï¼Œé€‰æ‹©æ¶æ„å®é™…ä¸Šæ„å‘³ç€é€‰æ‹©ä¸€ä¸ªä¸åŒæ¨¡å‹çš„ç³»åˆ—ï¼ˆæ¯ä¸ªæ¨¡å‹è¶…å‚æ•°è®¾ç½®å¯¹åº”ä¸€ä¸ªæ¨¡å‹ï¼‰ã€‚
  - æˆ‘ä»¬å°†åœ¨[é€‰æ‹©åˆå§‹é…ç½®](#choosing-the-initial-configuration)å’Œ[ç§‘å­¦æ–¹æ³•æ”¹è¿›æ¨¡å‹æ€§èƒ½](#a-scientific-approach-to-improving-model-performance)ä¸­è€ƒè™‘é€‰æ‹©æ¨¡å‹è¶…å‚æ•°çš„é—®é¢˜ã€‚
- åœ¨å¯èƒ½çš„æƒ…å†µä¸‹ï¼Œå°½é‡æ‰¾åˆ°ä¸€ç¯‡ä¸æ‰‹å¤´é—®é¢˜å°½å¯èƒ½æ¥è¿‘çš„è®ºæ–‡ï¼Œå¹¶ä»¥æ­¤æ¨¡å‹ä½œä¸ºèµ·ç‚¹è¿›è¡Œå¤ç°ã€‚

### Choosing the optimizer

**_é€‰æ‹©ä¼˜åŒ–å™¨_**

**_æ¦‚è¦ï¼š_** _æ ¹æ®æ‰‹å¤´é—®é¢˜çš„ç±»å‹ï¼Œé¦–å…ˆé€‰æ‹©æœ€æµè¡Œçš„ä¼˜åŒ–å™¨ã€‚_

- æ²¡æœ‰ä¸€ç§ä¼˜åŒ–å™¨é€‚ç”¨äºæ‰€æœ‰ç±»å‹çš„æœºå™¨å­¦ä¹ é—®é¢˜å’Œæ¨¡å‹æ¶æ„ã€‚å³ä¾¿æ˜¯[æ¯”è¾ƒä¼˜åŒ–å™¨æ€§èƒ½ä¹Ÿæ˜¯ä¸€é¡¹å›°éš¾çš„ä»»åŠ¡](https://arxiv.org/abs/1910.05446)ã€‚ğŸ¤–
- æˆ‘ä»¬å»ºè®®åœ¨å¼€å§‹æ–°é¡¹ç›®æ—¶ï¼ŒåšæŒä½¿ç”¨ç»éªŒä¸°å¯Œã€æµè¡Œçš„ä¼˜åŒ–å™¨ã€‚
  - ç†æƒ³æƒ…å†µä¸‹ï¼Œé€‰æ‹©ç”¨äºç›¸åŒç±»å‹é—®é¢˜çš„æœ€æµè¡Œçš„ä¼˜åŒ–å™¨ã€‚
- å‡†å¤‡å¥½å…³æ³¨æ‰€é€‰æ‹©ä¼˜åŒ–å™¨çš„ **\*æ‰€æœ‰\*** è¶…å‚æ•°ã€‚
  - å…·æœ‰æ›´å¤šè¶…å‚æ•°çš„ä¼˜åŒ–å™¨å¯èƒ½éœ€è¦æ›´å¤šè°ƒæ•´å·¥ä½œï¼Œä»¥æ‰¾åˆ°æœ€ä½³é…ç½®ã€‚
  - åœ¨é¡¹ç›®çš„åˆæœŸé˜¶æ®µï¼Œå½“æˆ‘ä»¬è¯•å›¾æ‰¾åˆ°å„ç§å…¶ä»–è¶…å‚æ•°çš„æœ€ä½³å€¼æ—¶ï¼ˆä¾‹å¦‚æ¶æ„è¶…å‚æ•°ï¼‰ï¼Œå°¤å…¶éœ€è¦å…³æ³¨æ­¤é—®é¢˜ï¼ŒåŒæ—¶å°†ä¼˜åŒ–å™¨è¶…å‚æ•°è§†ä¸º[æ— å…³å‚æ•°](#identifying-scientific-nuisance-and-fixed-hyperparameters).
  - åœ¨é¡¹ç›®åˆæœŸï¼Œä½¿ç”¨ä¸€ä¸ªæ›´ç®€å•çš„ä¼˜åŒ–å™¨ï¼ˆä¾‹å¦‚å…·æœ‰å›ºå®šåŠ¨é‡çš„ SGD æˆ–å…·æœ‰å›ºå®š $\epsilon$ ã€ $\beta_{1}$ å’Œ $\beta_{2}$ çš„ Adamï¼‰å¯èƒ½æ›´ä¸ºå¯å–ï¼Œéšåå†åˆ‡æ¢åˆ°æ›´é€šç”¨çš„ä¼˜åŒ–å™¨ã€‚
- æˆ‘ä»¬å–œæ¬¢çš„ä¸€äº›ç»éªŒä¸°å¯Œçš„ä¼˜åŒ–å™¨åŒ…æ‹¬ï¼ˆä½†ä¸é™äºï¼‰ï¼š
  - [å¸¦æœ‰åŠ¨é‡çš„ SGD](#what-are-the-update-rules-for-all-the-popular-optimization-algorithms)ï¼ˆæˆ‘ä»¬å–œæ¬¢ Nesterov çš„å˜ä½“ï¼‰
  - [Adam å’Œ NAdam](#what-are-the-update-rules-for-all-the-popular-optimization-algorithms)ï¼Œæ¯”å¸¦æœ‰åŠ¨é‡çš„ SGD æ›´é€šç”¨ã€‚è¯·æ³¨æ„ï¼ŒAdam æœ‰ 4 ä¸ªå¯è°ƒè¶…å‚æ•°ï¼Œ[å®ƒä»¬éƒ½å¯èƒ½å¾ˆé‡è¦](https://arxiv.org/abs/1910.05446)ï¼
    - è¯·å‚è§[Adam çš„è¶…å‚æ•°åº”è¯¥å¦‚ä½•è°ƒæ•´ï¼Ÿ](#how-should-adams-hyperparameters-be-tuned)

### Choosing the batch size

**_é€‰æ‹©æ‰¹é‡å¤§å°_**

**_æ¦‚è¦ï¼š_** _æ‰¹é‡å¤§å°æ§åˆ¶ç€è®­ç»ƒé€Ÿåº¦ï¼Œä¸åº”ç›´æ¥ç”¨äºè°ƒæ•´éªŒè¯é›†çš„æ€§èƒ½ã€‚é€šå¸¸ï¼Œç†æƒ³çš„æ‰¹é‡å¤§å°å°†æ˜¯ç¡¬ä»¶æ”¯æŒçš„æœ€å¤§æ‰¹é‡å¤§å°ã€‚_

- æ‰¹é‡å¤§å°æ˜¯ç¡®å®š _è®­ç»ƒæ—¶é—´_ å’Œ _è®¡ç®—èµ„æºæ¶ˆè€—_ çš„å…³é”®å› ç´ ã€‚
- å¢åŠ æ‰¹é‡å¤§å°é€šå¸¸ä¼šå‡å°‘è®­ç»ƒæ—¶é—´ã€‚è¿™é€šå¸¸æ˜¯éå¸¸æœ‰ç›Šçš„ï¼Œå› ä¸ºå®ƒï¼Œä¾‹å¦‚ï¼š
  - åœ¨å›ºå®šçš„æ—¶é—´é—´éš”å†…ï¼Œå…è®¸æ›´å…¨é¢åœ°è°ƒæ•´è¶…å‚æ•°ï¼Œæœ‰å¯èƒ½å¾—åˆ°æ›´å¥½çš„æœ€ç»ˆæ¨¡å‹ã€‚
  - å‡å°‘å¼€å‘å‘¨æœŸçš„å»¶è¿Ÿï¼Œä½¿å¾—æ–°çš„æƒ³æ³•èƒ½å¤Ÿæ›´é¢‘ç¹åœ°è¿›è¡Œæµ‹è¯•ã€‚
- å¢åŠ æ‰¹é‡å¤§å°å¯èƒ½ä¼šå¯¼è‡´èµ„æºæ¶ˆè€—å‡å°‘ã€å¢åŠ æˆ–ä¸å˜ã€‚
- æ‰¹é‡å¤§å° _ä¸åº”è¢«_ è§†ä¸ºå¯è°ƒæ•´çš„è¶…å‚æ•°ï¼Œç”¨äºéªŒè¯é›†æ€§èƒ½ã€‚
  - åªè¦æ‰€æœ‰çš„è¶…å‚æ•°éƒ½è¢«å¾ˆå¥½åœ°è°ƒæ•´ï¼ˆå°¤å…¶æ˜¯å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–è¶…å‚æ•°ï¼‰ï¼Œå¹¶ä¸”è®­ç»ƒæ­¥æ•°è¶³å¤Ÿå¤šï¼Œä½¿ç”¨ä»»ä½•æ‰¹é‡å¤§å°éƒ½åº”è¯¥èƒ½å¤Ÿè¾¾åˆ°ç›¸åŒçš„æœ€ç»ˆæ€§èƒ½ï¼ˆå‚è§[Shallue et al. 2018](https://arxiv.org/abs/1811.03600)ï¼‰ã€‚
  - è¯·å‚é˜…[ä¸ºä»€ä¹ˆä¸åº”è¯¥è°ƒæ•´æ‰¹é‡å¤§å°ä»¥ç›´æ¥æé«˜éªŒè¯é›†æ€§èƒ½ï¼Ÿ](#why-shouldnt-the-batch-size-be-tuned-to-directly-improve-validation-set-performance)

#### Determining the feasible batch sizes and estimating training throughput

**_ç¡®å®šå¯è¡Œçš„æ‰¹é‡å¤§å°å’Œä¼°è®¡è®­ç»ƒååé‡_**

<details><summary><em>[ç‚¹å‡»å±•å¼€]</em></summary>

<br>

- å¯¹äºç»™å®šçš„æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼Œé€šå¸¸ä¼šæœ‰ä¸€ç³»åˆ—å—åˆ°å¯ç”¨ç¡¬ä»¶æ”¯æŒçš„æ‰¹é‡å¤§å°ã€‚é™åˆ¶å› ç´ é€šå¸¸æ˜¯åŠ é€Ÿå™¨å†…å­˜ã€‚
- ä¸å¹¸çš„æ˜¯ï¼Œåœ¨ä¸è¿è¡Œæˆ–è‡³å°‘ç¼–è¯‘å®Œæ•´ä¸ªè®­ç»ƒç¨‹åºçš„æƒ…å†µä¸‹ï¼Œå¾ˆéš¾è®¡ç®—å“ªäº›æ‰¹é‡å¤§å°å°†é€‚åº”å†…å­˜ã€‚
- æœ€ç®€å•çš„è§£å†³æ–¹æ¡ˆé€šå¸¸æ˜¯ä»¥ä¸åŒçš„æ‰¹é‡å¤§å°è¿è¡Œè®­ç»ƒä½œä¸šï¼ˆä¾‹å¦‚ï¼Œå¢åŠ çš„ 2 çš„å¹‚ï¼‰ï¼Œç›´åˆ°å…¶ä¸­ä¸€ä¸ªä½œä¸šè¶…å‡ºå¯ç”¨å†…å­˜ã€‚
- å¯¹äºæ¯ä¸ªæ‰¹é‡å¤§å°ï¼Œæˆ‘ä»¬åº”è¯¥è®­ç»ƒè¶³å¤Ÿé•¿çš„æ—¶é—´ï¼Œä»¥è·å¾— _è®­ç»ƒååé‡_ çš„å¯é ä¼°ç®—ã€‚

<p align="center">è®­ç»ƒååé‡ =ï¼ˆæ¯ç§’å¤„ç†çš„ç¤ºä¾‹æ•°ï¼‰</p>

<p align="center">æˆ–è€…ç­‰ä»·åœ°ï¼Œ<em>æ¯æ­¥çš„æ—¶é—´</em>ã€‚</p>

<p align="center">æ¯æ­¥çš„æ—¶é—´ =ï¼ˆæ‰¹é‡å¤§å°ï¼‰/ï¼ˆè®­ç»ƒååé‡ï¼‰</p>

- å½“åŠ é€Ÿå™¨å°šæœªé¥±å’Œæ—¶ï¼Œå¦‚æœæ‰¹é‡å¤§å°ç¿»å€ï¼Œè®­ç»ƒååé‡ä¹Ÿåº”è¯¥ç¿»å€ï¼ˆæˆ–è€…è‡³å°‘å‡ ä¹ç¿»å€ï¼‰ã€‚ç­‰ä»·åœ°ï¼Œéšç€æ‰¹é‡å¤§å°çš„å¢åŠ ï¼Œæ¯æ­¥çš„æ—¶é—´åº”è¯¥ä¿æŒä¸å˜ï¼ˆæˆ–è€…è‡³å°‘å‡ ä¹ä¸å˜ï¼‰ã€‚
- å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œé‚£ä¹ˆè®­ç»ƒæµæ°´çº¿å¯èƒ½å­˜åœ¨ç“¶é¢ˆï¼Œæ¯”å¦‚ I/O æˆ–è®¡ç®—èŠ‚ç‚¹ä¹‹é—´çš„åŒæ­¥ã€‚åœ¨ç»§ç»­ä¹‹å‰ï¼Œå¯èƒ½éœ€è¦è¯Šæ–­å’Œçº æ­£è¿™ä¸ªé—®é¢˜ã€‚
- å¦‚æœè®­ç»ƒååé‡åªå¢åŠ åˆ°æŸä¸ªæœ€å¤§æ‰¹é‡å¤§å°ï¼Œé‚£ä¹ˆæˆ‘ä»¬åº”è¯¥åªè€ƒè™‘æœ€å¤§æ‰¹é‡å¤§å°ï¼Œå³ä½¿ç¡¬ä»¶æ”¯æŒæ›´å¤§çš„æ‰¹é‡å¤§å°ã€‚
  - ä½¿ç”¨æ›´å¤§æ‰¹é‡å¤§å°çš„æ‰€æœ‰å¥½å¤„éƒ½åŸºäºè®­ç»ƒååé‡çš„å¢åŠ ã€‚å¦‚æœæ²¡æœ‰å¢åŠ ï¼Œä¿®å¤ç“¶é¢ˆæˆ–ä½¿ç”¨è¾ƒå°çš„æ‰¹é‡å¤§å°ã€‚
  - **æ¢¯åº¦ç´¯ç§¯** æ¨¡æ‹Ÿçš„æ˜¯æ¯”ç¡¬ä»¶æ”¯æŒæ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œå› æ­¤å¹¶ä¸æä¾›ä»»ä½•ååé‡çš„å¥½å¤„ã€‚åœ¨åº”ç”¨å·¥ä½œä¸­é€šå¸¸åº”è¯¥é¿å…ä½¿ç”¨ã€‚
- è¿™äº›æ­¥éª¤å¯èƒ½éœ€è¦åœ¨æ¯æ¬¡æ›´æ”¹æ¨¡å‹æˆ–ä¼˜åŒ–å™¨æ—¶é‡å¤ï¼ˆä¾‹å¦‚ï¼Œä¸åŒçš„æ¨¡å‹æ¶æ„å¯èƒ½å…è®¸æ›´å¤§çš„æ‰¹é‡å¤§å°é€‚åº”å†…å­˜ï¼‰ã€‚

</details>

#### Choosing the batch size to minimize training time

**_é€‰æ‹©æ‰¹é‡å¤§å°ä»¥æœ€å°åŒ–è®­ç»ƒæ—¶é—´_**

<details><summary><em>[ç‚¹å‡»å±•å¼€]</em></summary>

<br>

<p align="center">è®­ç»ƒæ—¶é—´ = ï¼ˆæ¯æ­¥çš„æ—¶é—´ï¼‰ x ï¼ˆæ€»æ­¥æ•°ï¼‰</p>

- æˆ‘ä»¬é€šå¸¸å¯ä»¥è®¤ä¸ºæ¯æ­¥çš„æ—¶é—´å¯¹äºæ‰€æœ‰å¯è¡Œçš„æ‰¹é‡å¤§å°æ¥è¯´è¿‘ä¼¼æ˜¯æ’å®šçš„ã€‚å½“æ²¡æœ‰å¹¶è¡Œè®¡ç®—çš„é¢å¤–å¼€é”€ï¼Œå¹¶ä¸”æ‰€æœ‰è®­ç»ƒç“¶é¢ˆéƒ½å·²è¢«è¯Šæ–­å’Œçº æ­£æ—¶ï¼Œè¿™æ˜¯æ­£ç¡®çš„ï¼ˆå‚è§[å‰ä¸€èŠ‚](#determining-the-feasible-batch-sizes-and-estimating-training-throughput)å…³äºå¦‚ä½•è¯†åˆ«è®­ç»ƒç“¶é¢ˆçš„å†…å®¹ï¼‰ã€‚å®é™…ä¸Šï¼Œé€šå¸¸å¢åŠ æ‰¹é‡å¤§å°ä¼šå¸¦æ¥ä¸€äº›é¢å¤–çš„å¼€é”€ã€‚
- éšç€æ‰¹é‡å¤§å°çš„å¢åŠ ï¼Œé€šå¸¸éœ€è¦è¾¾åˆ°å›ºå®šæ€§èƒ½ç›®æ ‡çš„æ€»æ­¥æ•°ä¼šå‡å°‘ï¼ˆå‰ææ˜¯åœ¨æ›´æ”¹æ‰¹é‡å¤§å°æ—¶é‡æ–°è°ƒæ•´æ‰€æœ‰ç›¸å…³çš„è¶…å‚æ•°ï¼›[Shallue et al. 2018](https://arxiv.org/abs/1811.03600)ï¼‰ã€‚
  - ä¾‹å¦‚ï¼Œå°†æ‰¹é‡å¤§å°ç¿»å€å¯èƒ½ä¼šå°†æ‰€éœ€çš„æ€»æ­¥æ•°å‡åŠã€‚è¿™è¢«ç§°ä¸º**å®Œç¾æ‰©å±•**ã€‚
  - å®Œç¾æ‰©å±•åœ¨æ‰¹é‡å¤§å°è¾¾åˆ°ä¸´ç•Œæ‰¹é‡å¤§å°ä¹‹å‰éƒ½æˆç«‹ï¼Œæ­¤åä¼šè·å¾—é€’å‡çš„å›æŠ¥ã€‚
  - æœ€ç»ˆï¼Œå¢åŠ æ‰¹é‡å¤§å°ä¸å†å‡å°‘è®­ç»ƒæ­¥æ•°ï¼ˆä½†æ°¸è¿œä¸ä¼šå¢åŠ ï¼‰ã€‚
- å› æ­¤ï¼Œæœ€å°åŒ–è®­ç»ƒæ—¶é—´çš„æ‰¹é‡å¤§å°é€šå¸¸æ˜¯ä»ç„¶å‡å°‘æ‰€éœ€è®­ç»ƒæ­¥æ•°çš„æœ€å¤§æ‰¹é‡å¤§å°ã€‚
  - è¿™ä¸ªæ‰¹é‡å¤§å°å–å†³äºæ•°æ®é›†ã€æ¨¡å‹å’Œä¼˜åŒ–å™¨ï¼Œå¦‚ä½•è®¡ç®—å®ƒæ˜¯ä¸€ä¸ªå°šæœªè§£å†³çš„é—®é¢˜ï¼Œé™¤éåœ¨æ¯ä¸ªæ–°é—®é¢˜ä¸Šè¿›è¡Œå®éªŒæ€§çš„å‘ç°ã€‚ğŸ¤–
  - åœ¨æ¯”è¾ƒæ‰¹é‡å¤§å°æ—¶ï¼Œè¦æ³¨æ„ç¤ºä¾‹é¢„ç®—/[è¿­ä»£](https://developers.google.com/machine-learning/glossary#epoch)é¢„ç®—ï¼ˆåœ¨å›ºå®šè®­ç»ƒç¤ºä¾‹å±•ç¤ºæ¬¡æ•°çš„åŒæ—¶è¿è¡Œæ‰€æœ‰å®éªŒï¼‰ä¸æ­¥éª¤é¢„ç®—ï¼ˆåœ¨å›ºå®šè®­ç»ƒæ­¥éª¤æ•°çš„åŒæ—¶è¿è¡Œæ‰€æœ‰å®éªŒï¼‰ä¹‹é—´çš„åŒºåˆ«ã€‚
    - é€šè¿‡è¿­ä»£é¢„ç®—æ¯”è¾ƒæ‰¹é‡å¤§å°åªæ¢ç©¶äº†å®Œç¾æ‰©å±•çš„èŒƒå›´ï¼Œå³ä½¿æ›´å¤§çš„æ‰¹é‡å¤§å°å¯èƒ½é€šè¿‡å‡å°‘æ‰€éœ€çš„è®­ç»ƒæ­¥éª¤è€Œæä¾›æœ‰æ„ä¹‰çš„åŠ é€Ÿã€‚
  - é€šå¸¸ï¼Œå¯ç”¨ç¡¬ä»¶æ”¯æŒçš„æœ€å¤§æ‰¹é‡å¤§å°å¯èƒ½ä¼šå°äºä¸´ç•Œæ‰¹é‡å¤§å°ã€‚å› æ­¤ï¼Œä¸€ä¸ªå¥½çš„ç»éªŒæ³•åˆ™ï¼ˆåœ¨æ²¡æœ‰è¿è¡Œä»»ä½•å®éªŒçš„æƒ…å†µä¸‹ï¼‰æ˜¯ä½¿ç”¨å¯èƒ½çš„æœ€å¤§æ‰¹é‡å¤§å°ã€‚
- å¦‚æœä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°æœ€ç»ˆå¯¼è‡´è®­ç»ƒæ—¶é—´å¢åŠ ï¼Œé‚£ä¹ˆä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°å°±æ²¡æœ‰æ„ä¹‰ã€‚

</details>

#### Choosing the batch size to minimize resource consumption

**_é€‰æ‹©æ‰¹é‡å¤§å°ä»¥æœ€å°åŒ–èµ„æºæ¶ˆè€—_**

<details><summary><em>[ç‚¹å‡»å±•å¼€]</em></summary>

<br>

- å¢åŠ æ‰¹é‡å¤§å°æ¶‰åŠåˆ°ä¸¤ç§ç±»å‹çš„èµ„æºæˆæœ¬ï¼š
  1.  _å‰æœŸæˆæœ¬_ï¼Œä¾‹å¦‚è´­ä¹°æ–°ç¡¬ä»¶æˆ–é‡å†™è®­ç»ƒæµæ°´çº¿ä»¥å®ç°å¤š GPU / å¤š TPU è®­ç»ƒã€‚
  2.  _ä½¿ç”¨æˆæœ¬_ï¼Œä¾‹å¦‚è®¡è´¹ä¸å›¢é˜Ÿçš„èµ„æºé¢„ç®—ç›¸æŠµï¼Œæ¥è‡ªäº‘æœåŠ¡æä¾›å•†çš„è®¡è´¹ï¼Œä»¥åŠç”µåŠ›/ç»´æŠ¤æˆæœ¬ã€‚
- å¦‚æœå¢åŠ æ‰¹é‡å¤§å°å­˜åœ¨æ˜¾è‘—çš„å‰æœŸæˆæœ¬ï¼Œå¯èƒ½æœ€å¥½æ¨è¿Ÿå¢åŠ æ‰¹é‡å¤§å°ï¼Œç›´åˆ°é¡¹ç›®æˆç†Ÿï¼Œæ›´å®¹æ˜“è¯„ä¼°æˆæœ¬æ•ˆç›Šçš„æƒè¡¡ã€‚å®æ–½å¤šä¸»æœºå¹¶è¡Œè®­ç»ƒç¨‹åºå¯èƒ½å¼•å…¥[é”™è¯¯](#considerations-for-multi-host-pipelines)å’Œ[ç»†å¾®é—®é¢˜](#batch-normalization-implementation-details)ï¼Œå› æ­¤å¯èƒ½æœ€å¥½ä¸€å¼€å§‹å°±ä½¿ç”¨æ›´ç®€å•çš„æµæ°´çº¿ã€‚ï¼ˆå¦ä¸€æ–¹é¢ï¼Œåœ¨éœ€è¦å¤§é‡è°ƒæ•´å®éªŒçš„æ—©æœŸé˜¶æ®µï¼Œè®­ç»ƒæ—¶é—´çš„å¤§å¹…æé€Ÿå¯èƒ½æ˜¯éå¸¸æœ‰ç›Šçš„ã€‚ï¼‰
- æˆ‘ä»¬å°†æ€»ä½¿ç”¨æˆæœ¬ï¼ˆå¯èƒ½åŒ…æ‹¬å¤šç§ä¸åŒç±»å‹çš„æˆæœ¬ï¼‰ç§°ä¸º "èµ„æºæ¶ˆè€—"ã€‚æˆ‘ä»¬å¯ä»¥å°†èµ„æºæ¶ˆè€—åˆ†è§£ä¸ºä»¥ä¸‹ç»„æˆéƒ¨åˆ†ï¼š

<p align="center">èµ„æºæ¶ˆè€— =ï¼ˆæ¯æ­¥çš„èµ„æºæ¶ˆè€—ï¼‰ xï¼ˆæ€»æ­¥æ•°ï¼‰</p>

- å¢åŠ æ‰¹é‡å¤§å°é€šå¸¸å…è®¸æˆ‘ä»¬[å‡å°‘æ€»æ­¥æ•°](#choosing-the-batch-size-to-minimize-training-time)ã€‚èµ„æºæ¶ˆè€—æ˜¯å¢åŠ è¿˜æ˜¯å‡å°‘å°†å–å†³äºæ¯æ­¥æ¶ˆè€—çš„å˜åŒ–ã€‚
  - å¢åŠ æ‰¹é‡å¤§å°å¯èƒ½ä¼š _å‡å°‘_ èµ„æºæ¶ˆè€—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¾ƒå¤§æ‰¹é‡å¤§å°çš„æ¯ä¸ªæ­¥éª¤å¯ä»¥åœ¨ä¸è¾ƒå°æ‰¹é‡å¤§å°ç›¸åŒçš„ç¡¬ä»¶ä¸Šè¿è¡Œï¼ˆæ¯æ­¥åªå¢åŠ ä¸€ç‚¹æ—¶é—´ï¼‰ï¼Œé‚£ä¹ˆæ¯æ­¥èµ„æºæ¶ˆè€—çš„å¢åŠ å¯èƒ½ä¼šè¢«æ­¥éª¤æ•°é‡çš„å‡å°‘æ‰€æŠµæ¶ˆã€‚
  - å¢åŠ æ‰¹é‡å¤§å°å¯èƒ½ _ä¸ä¼šæ”¹å˜_ èµ„æºæ¶ˆè€—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå°†æ‰¹é‡å¤§å°ç¿»å€ä¼šå°†æ‰€éœ€æ­¥éª¤æ•°é‡å‡åŠå¹¶ä¸”ä½¿ç”¨çš„ GPU æ•°é‡ç¿»å€ï¼Œé‚£ä¹ˆæ€»çš„æ¶ˆè€—ï¼ˆä»¥ GPU å°æ—¶ä¸ºå•ä½ï¼‰å°†ä¿æŒä¸å˜ã€‚
  - å¢åŠ æ‰¹é‡å¤§å°å¯èƒ½ä¼š _å¢åŠ _ èµ„æºæ¶ˆè€—ã€‚ä¾‹å¦‚ï¼Œå¦‚æœå¢åŠ æ‰¹é‡å¤§å°éœ€è¦å‡çº§ç¡¬ä»¶ï¼Œæ¯æ­¥æ¶ˆè€—çš„å¢åŠ å¯èƒ½ä¼šè¶…è¿‡å‡å°‘çš„æ­¥éª¤æ•°é‡ã€‚

</details>

#### Changing the batch size requires re-tuning most hyperparameters

**_æ›´æ”¹æ‰¹é‡å¤§å°éœ€è¦é‡æ–°è°ƒæ•´å¤§å¤šæ•°è¶…å‚æ•°_**

<details><summary><em>[ç‚¹å‡»å±•å¼€]</em></summary>

<br>

- å¤§å¤šæ•°è¶…å‚æ•°çš„æœ€ä½³å€¼å¯¹æ‰¹é‡å¤§å°æ•æ„Ÿã€‚å› æ­¤ï¼Œæ›´æ”¹æ‰¹é‡å¤§å°é€šå¸¸éœ€è¦é‡æ–°å¼€å§‹è°ƒæ•´è¿‡ç¨‹ã€‚
- ä¸æ‰¹é‡å¤§å°äº¤äº’ä½œç”¨æœ€å¼ºã€å› æ­¤å¯¹æ¯ä¸ªæ‰¹é‡å¤§å°åˆ†åˆ«è¿›è¡Œè°ƒæ•´æœ€ä¸ºé‡è¦çš„è¶…å‚æ•°æ˜¯ä¼˜åŒ–å™¨è¶…å‚æ•°ï¼ˆä¾‹å¦‚å­¦ä¹ ç‡ã€åŠ¨é‡ï¼‰å’Œæ­£åˆ™åŒ–è¶…å‚æ•°ã€‚
- åœ¨é¡¹ç›®å¼€å§‹æ—¶é€‰æ‹©æ‰¹é‡å¤§å°æ—¶è¦è®°ä½è¿™ä¸€ç‚¹ã€‚å¦‚æœä»¥åéœ€è¦åˆ‡æ¢åˆ°ä¸åŒçš„æ‰¹é‡å¤§å°ï¼Œé‡æ–°ä¸ºæ–°çš„æ‰¹é‡å¤§å°è°ƒæ•´æ‰€æœ‰å‚æ•°å¯èƒ½ä¼šå›°éš¾ã€è€—æ—¶ä¸”æ˜‚è´µã€‚

</details>

#### How batch norm interacts with the batch size

**_æ‰¹é‡å½’ä¸€åŒ–ä¸æ‰¹é‡å¤§å°çš„äº¤äº’_**

<details><summary><em>[ç‚¹å‡»å±•å¼€]</em></summary>

<br>

- æ‰¹é‡å½’ä¸€åŒ–æ˜¯å¤æ‚çš„ï¼Œé€šå¸¸åº”è¯¥ä½¿ç”¨ä¸æ¢¯åº¦è®¡ç®—ä¸åŒçš„æ‰¹é‡å¤§å°æ¥è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ã€‚æœ‰å…³è¯¦ç»†è®¨è®ºï¼Œè¯·å‚è§[æ‰¹é‡å½’ä¸€åŒ–éƒ¨åˆ†](#batch-normalization-implementation-details)ã€‚

</details>

### Choosing the initial configuration

**_é€‰æ‹©åˆå§‹é…ç½®_**

- åœ¨å¼€å§‹è¶…å‚æ•°è°ƒæ•´ä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®å®šèµ·ç‚¹ã€‚è¿™åŒ…æ‹¬æŒ‡å®šï¼ˆ1ï¼‰æ¨¡å‹é…ç½®ï¼ˆä¾‹å¦‚ï¼Œå±‚æ•°ï¼‰ï¼Œï¼ˆ2ï¼‰ä¼˜åŒ–å™¨è¶…å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œå­¦ä¹ ç‡ï¼‰ï¼Œä»¥åŠï¼ˆ3ï¼‰è®­ç»ƒæ­¥æ•°ã€‚
- ç¡®å®šè¿™ä¸ªåˆå§‹é…ç½®å°†éœ€è¦ä¸€äº›æ‰‹åŠ¨é…ç½®çš„è®­ç»ƒè¿è¡Œå’Œåå¤å°è¯•ã€‚
- æˆ‘ä»¬çš„æŒ‡å¯¼åŸåˆ™æ˜¯æ‰¾åˆ°ä¸€ä¸ªç®€å•ã€ç›¸å¯¹å¿«é€Ÿã€ç›¸å¯¹ä½èµ„æºæ¶ˆè€—çš„é…ç½®ï¼Œä»¥è·å¾—ä¸€ä¸ªâ€œåˆç†â€çš„ç»“æœã€‚
  - â€œç®€å•â€æ„å‘³ç€å°½å¯èƒ½é¿å…ç‚«è€€ï¼Œè¿™äº›å¯ä»¥éšæ—¶æ·»åŠ ã€‚å³ä½¿ç‚«è€€åœ¨åé¢è¯æ˜æœ‰å¸®åŠ©ï¼Œå°†å®ƒä»¬æ·»åŠ åˆ°åˆå§‹é…ç½®ä¸­å¯èƒ½ä¼šæµªè´¹æ—¶é—´è°ƒæ•´æ— ç”¨çš„ç‰¹æ€§å’Œ/æˆ–å¼•å…¥ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚
    - ä¾‹å¦‚ï¼Œåœ¨æ·»åŠ ç‚«è€€çš„è¡°å‡è®¡åˆ’ä¹‹å‰ï¼Œä»ä¸€ä¸ªæ’å®šçš„å­¦ä¹ ç‡å¼€å§‹ã€‚
  - é€‰æ‹©ä¸€ä¸ªåˆå§‹é…ç½®ï¼Œæ—¢å¿«é€Ÿåˆæ¶ˆè€—æœ€å°‘èµ„æºï¼Œå°†ä½¿è¶…å‚æ•°è°ƒæ•´æ›´åŠ é«˜æ•ˆã€‚
    - ä¾‹å¦‚ï¼Œä»ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹å¼€å§‹ã€‚
  - â€œåˆç†â€çš„æ€§èƒ½å–å†³äºé—®é¢˜ï¼Œä½†è‡³å°‘æ„å‘³ç€è®­ç»ƒåçš„æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šæ¯”éšæœºæœºä¼šè¡¨ç°å¾—æ›´å¥½ï¼ˆå°½ç®¡å¯èƒ½ä¸è¶³ä»¥å€¼å¾—éƒ¨ç½²ï¼‰ã€‚
- é€‰æ‹©è®­ç»ƒæ­¥æ•°æ¶‰åŠå¹³è¡¡ä»¥ä¸‹å¼ åŠ›ï¼š
  - ä¸€æ–¹é¢ï¼Œæ›´å¤šçš„æ­¥æ•°è®­ç»ƒå¯ä»¥æé«˜æ€§èƒ½ï¼Œå¹¶ä½¿è¶…å‚æ•°è°ƒæ•´æ›´å®¹æ˜“ï¼ˆå‚è§[Shallue et al. 2018](https://arxiv.org/abs/1811.03600)ï¼‰ã€‚
  - å¦ä¸€æ–¹é¢ï¼Œè¾ƒå°‘çš„æ­¥æ•°è®­ç»ƒæ„å‘³ç€æ¯æ¬¡è®­ç»ƒè¿è¡Œæ›´å¿«ï¼Œä½¿ç”¨æ›´å°‘çš„èµ„æºï¼Œé€šè¿‡å‡å°‘å‘¨æœŸä¹‹é—´çš„æ—¶é—´å¹¶å…è®¸æ›´å¤šå®éªŒå¹¶è¡Œè¿è¡Œï¼Œæé«˜è°ƒæ•´æ•ˆç‡ã€‚æ­¤å¤–ï¼Œå¦‚æœæœ€åˆé€‰æ‹©äº†ä¸€ä¸ªä¸å¿…è¦å¤§çš„æ­¥éª¤é¢„ç®—ï¼Œå¯èƒ½å¾ˆéš¾åœ¨åæœŸæ›´æ”¹ï¼Œä¾‹å¦‚ï¼Œä¸€æ—¦å­¦ä¹ ç‡è®¡åˆ’ä¸ºé‚£ä¸ªæ­¥æ•°è°ƒæ•´ã€‚

## A scientific approach to improving model performance

**_æå‡æ¨¡å‹æ€§èƒ½çš„ç§‘å­¦æ–¹æ³•_**

åœ¨æœ¬æ–‡æ¡£ä¸­ï¼Œæœºå™¨å­¦ä¹ å¼€å‘çš„ç»ˆæç›®æ ‡æ˜¯æœ€å¤§åŒ–éƒ¨ç½²æ¨¡å‹çš„æ•ˆç”¨ã€‚å°½ç®¡å¼€å‘è¿‡ç¨‹çš„è®¸å¤šæ–¹é¢åœ¨ä¸åŒåº”ç”¨ä¹‹é—´å¯èƒ½å­˜åœ¨å·®å¼‚ï¼ˆä¾‹å¦‚ï¼Œæ—¶é—´é•¿åº¦ã€å¯ç”¨è®¡ç®—èµ„æºã€æ¨¡å‹ç±»å‹ï¼‰ï¼Œä½†æˆ‘ä»¬é€šå¸¸å¯ä»¥åœ¨ä»»ä½•é—®é¢˜ä¸Šä½¿ç”¨ç›¸åŒçš„åŸºæœ¬æ­¥éª¤å’ŒåŸåˆ™ã€‚

æˆ‘ä»¬çš„æŒ‡å¯¼åŸºäºä»¥ä¸‹å‡è®¾ï¼š

- å·²ç»æœ‰ä¸€ä¸ªå®Œå…¨è¿è¡Œçš„è®­ç»ƒæµç¨‹ï¼Œä»¥åŠä¸€ä¸ªèƒ½å¤Ÿè·å¾—åˆç†ç»“æœçš„é…ç½®ã€‚
- æœ‰è¶³å¤Ÿçš„è®¡ç®—èµ„æºè¿›è¡Œæœ‰æ„ä¹‰çš„è°ƒæ•´å®éªŒï¼Œå¹¶å¯ä»¥å¹¶è¡Œè¿è¡Œè‡³å°‘å‡ ä¸ªè®­ç»ƒä½œä¸šã€‚

### The incremental tuning strategy

**_æ¸è¿›è°ƒæ•´ç­–ç•¥_**

**_æ¦‚è¦ï¼š_** _ä»ä¸€ä¸ªç®€å•çš„é…ç½®å¼€å§‹ï¼Œé€æ­¥è¿›è¡Œæ”¹è¿›ï¼ŒåŒæ—¶æ·±å…¥äº†è§£é—®é¢˜ã€‚ç¡®ä¿ä»»ä½•æ”¹è¿›éƒ½åŸºäºå……åˆ†çš„è¯æ®ï¼Œä»¥é¿å…æ·»åŠ ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚_

- æˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€ä¸ªæœ€å¤§åŒ–æ¨¡å‹æ€§èƒ½çš„é…ç½®ã€‚
  - åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨å›ºå®šçš„æˆªæ­¢æ—¥æœŸå‰æœ€å¤§åŒ–æˆ‘ä»¬å¯ä»¥æ”¹å–„æ¨¡å‹çš„ç¨‹åº¦ï¼ˆä¾‹å¦‚ï¼Œå‚åŠ ç«èµ›æäº¤ï¼‰ã€‚
  - åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›ä¸æ–­æ”¹è¿›æ¨¡å‹ï¼ˆä¾‹å¦‚ï¼ŒæŒç»­æ”¹è¿›åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨çš„æ¨¡å‹ï¼‰ã€‚
- åŸåˆ™ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ä½¿ç”¨ç®—æ³•è‡ªåŠ¨æœç´¢å¯èƒ½é…ç½®çš„æ•´ä¸ªç©ºé—´æ¥æœ€å¤§åŒ–æ€§èƒ½ï¼Œä½†è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå®é™…å¯è¡Œçš„é€‰é¡¹ã€‚
  - å¯èƒ½é…ç½®çš„ç©ºé—´éå¸¸åºå¤§ï¼Œç›®å‰è¿˜æ²¡æœ‰è¶³å¤Ÿå¤æ‚çš„ç®—æ³•èƒ½å¤Ÿåœ¨æ²¡æœ‰äººç±»å¼•å¯¼çš„æƒ…å†µä¸‹é«˜æ•ˆåœ°æœç´¢è¿™ä¸ªç©ºé—´ã€‚
- å¤§å¤šæ•°è‡ªåŠ¨æœç´¢ç®—æ³•ä¾èµ–äºæ‰‹åŠ¨è®¾è®¡çš„ _æœç´¢ç©ºé—´_ ï¼Œå®ƒå®šä¹‰äº†è¦æœç´¢çš„é…ç½®é›†ï¼Œè€Œè¿™äº›æœç´¢ç©ºé—´å¯èƒ½ç›¸å½“é‡è¦ã€‚
- æœ€æœ‰æ•ˆçš„æ–¹å¼æ˜¯ä»ä¸€ä¸ªç®€å•çš„é…ç½®å¼€å§‹ï¼Œé€æ­¥æ·»åŠ ç‰¹æ€§å¹¶è¿›è¡Œæ”¹è¿›ï¼ŒåŒæ—¶æ·±å…¥äº†è§£é—®é¢˜ã€‚
  - æˆ‘ä»¬åœ¨æ¯è½®è°ƒæ•´ä¸­ä½¿ç”¨è‡ªåŠ¨æœç´¢ç®—æ³•ï¼Œå¹¶éšç€æˆ‘ä»¬çš„ç†è§£å¢åŠ è€Œä¸æ–­æ›´æ–°æˆ‘ä»¬çš„æœç´¢ç©ºé—´ã€‚
- éšç€æˆ‘ä»¬çš„æ¢ç´¢ï¼Œæˆ‘ä»¬è‡ªç„¶ä¼šå‘ç°è¶Šæ¥è¶Šå¥½çš„é…ç½®ï¼Œå› æ­¤æˆ‘ä»¬çš„ "æœ€ä½³" æ¨¡å‹å°†ä¸æ–­æ”¹è¿›ã€‚
  - å½“æˆ‘ä»¬æ›´æ–°æˆ‘ä»¬çš„æœ€ä½³é…ç½®æ—¶ï¼ˆå¯èƒ½ä¸å®é™…ç”Ÿäº§æ¨¡å‹çš„å¯åŠ¨ç›¸å¯¹åº”ï¼Œä¹Ÿå¯èƒ½ä¸ç›¸å¯¹åº”ï¼‰ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º _å¯åŠ¨_ ã€‚
  - å¯¹äºæ¯æ¬¡å¯åŠ¨ï¼Œæˆ‘ä»¬å¿…é¡»ç¡®ä¿å˜åŒ–æ˜¯åŸºäºå……åˆ†çš„è¯æ®è€Œä¸ä»…ä»…æ˜¯åŸºäºå¹¸è¿é…ç½®çš„éšæœºæœºä¼šï¼Œä»¥é¿å…å‘è®­ç»ƒæµæ°´çº¿æ·»åŠ ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚

åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œæˆ‘ä»¬çš„æ¸è¿›è°ƒæ•´ç­–ç•¥åŒ…æ‹¬é‡å¤ä»¥ä¸‹å››ä¸ªæ­¥éª¤ï¼š

1. ä¸ºä¸‹ä¸€è½®å®éªŒç¡®å®šä¸€ä¸ªé€‚å½“èŒƒå›´çš„ç›®æ ‡ã€‚
2. è®¾è®¡å¹¶è¿è¡Œä¸€ç»„å®éªŒï¼Œä»¥å®ç°å‘è¿™ä¸ªç›®æ ‡è¿ˆè¿›ã€‚
3. ä»ç»“æœä¸­å­¦åˆ°æˆ‘ä»¬èƒ½å­¦åˆ°çš„ä¸œè¥¿ã€‚
4. è€ƒè™‘æ˜¯å¦å¯åŠ¨æ–°çš„æœ€ä½³é…ç½®ã€‚

æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†å°†æ›´è¯¦ç»†åœ°è€ƒè™‘è¿™ä¸€ç­–ç•¥ã€‚

### Exploration vs exploitation

**_æ¢ç´¢ä¸å¼€å‘_**

**_æ¦‚è¦ï¼š_** _å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡æ˜¯æ·±å…¥äº†è§£é—®é¢˜ã€‚_

- å°½ç®¡äººä»¬å¯èƒ½è®¤ä¸ºæˆ‘ä»¬ä¼šèŠ±è´¹å¤§éƒ¨åˆ†æ—¶é—´å°è¯•åœ¨éªŒè¯é›†ä¸Šæœ€å¤§åŒ–æ€§èƒ½ï¼Œä½†å®é™…ä¸Šæˆ‘ä»¬èŠ±è´¹å¤§éƒ¨åˆ†æ—¶é—´å°è¯•æ·±å…¥äº†è§£é—®é¢˜ï¼Œç›¸å¯¹è¾ƒå°‘çš„æ—¶é—´è´ªå©ªåœ°ä¸“æ³¨äºéªŒè¯é”™è¯¯ã€‚
  - æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬èŠ±è´¹å¤§éƒ¨åˆ†æ—¶é—´åœ¨ "æ¢ç´¢"ï¼Œåªæœ‰å°‘é‡æ—¶é—´åœ¨ "å¼€å‘"ã€‚
- ä»é•¿è¿œæ¥çœ‹ï¼Œå¦‚æœæˆ‘ä»¬æƒ³æœ€å¤§åŒ–æœ€ç»ˆæ€§èƒ½ï¼Œç†è§£é—®é¢˜è‡³å…³é‡è¦ã€‚ä¼˜å…ˆè€ƒè™‘æ·±å…¥äº†è§£è€Œä¸æ˜¯çŸ­æœŸæ”¶ç›Šå¯ä»¥å¸®åŠ©æˆ‘ä»¬ï¼š
  - é¿å…å¯åŠ¨ä»…å› å†å²å¶ç„¶æ€§è€Œå­˜åœ¨äºè¡¨ç°è‰¯å¥½è¿è¡Œä¸­çš„ä¸å¿…è¦çš„æ›´æ”¹ã€‚
  - ç¡®å®šéªŒè¯é”™è¯¯æœ€æ•æ„Ÿçš„è¶…å‚æ•°ï¼Œå“ªäº›è¶…å‚æ•°ä¹‹é—´çš„äº¤äº’æœ€å¤šï¼Œå› æ­¤éœ€è¦ä¸€èµ·é‡æ–°è°ƒæ•´ï¼Œå“ªäº›è¶…å‚æ•°ç›¸å¯¹ä¸å¤ªæ•æ„Ÿäºå…¶ä»–å˜åŒ–ï¼Œå› æ­¤å¯ä»¥åœ¨å°†æ¥çš„å®éªŒä¸­å›ºå®šã€‚
  - æå‡ºå°è¯•çš„æ½œåœ¨æ–°ç‰¹æ€§ï¼Œä¾‹å¦‚å¦‚æœè¿‡æ‹Ÿåˆæ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œåˆ™å°è¯•æ–°çš„æ­£åˆ™åŒ–å™¨ã€‚
  - è¯†åˆ«ä¸èµ·ä½œç”¨çš„ç‰¹æ€§ï¼Œå› æ­¤å¯ä»¥åˆ é™¤ï¼Œå‡å°‘æœªæ¥å®éªŒçš„å¤æ‚æ€§ã€‚
  - è¾¨åˆ«è¶…å‚æ•°è°ƒæ•´å¸¦æ¥çš„æ”¹è¿›ä½•æ—¶å¯èƒ½å·²ç»é¥±å’Œã€‚
  - ç¼©å°å›´ç»•æœ€ä½³å€¼çš„æœç´¢ç©ºé—´ï¼Œä»¥æé«˜è°ƒæ•´æ•ˆç‡ã€‚
- å½“æˆ‘ä»¬æœ€ç»ˆå‡†å¤‡å¥½è´ªå¿ƒæ—¶ï¼Œå³ä½¿å®éªŒå¯¹è°ƒæ•´é—®é¢˜çš„ç»“æ„ä¸æ˜¯æœ€å¤§ç¨‹åº¦çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥çº¯ç²¹å…³æ³¨éªŒè¯é”™è¯¯ã€‚

### Choosing the goal for the next round of experiments

**_é€‰æ‹©ä¸‹ä¸€è½®å®éªŒçš„ç›®æ ‡_**

**_æ¦‚è¦ï¼š_** _æ¯ä¸€è½®å®éªŒéƒ½åº”è¯¥æœ‰æ˜ç¡®çš„ç›®æ ‡ï¼Œå¹¶ä¸”èŒƒå›´è¶³å¤Ÿç‹­çª„ï¼Œä»¥ä¾¿å®éªŒèƒ½å¤ŸçœŸæ­£æœç€ç›®æ ‡å–å¾—è¿›å±•ã€‚_

- æ¯ä¸€è½®å®éªŒéƒ½åº”è¯¥æœ‰æ˜ç¡®çš„ç›®æ ‡ï¼Œå¹¶ä¸”èŒƒå›´è¶³å¤Ÿç‹­çª„ï¼Œä»¥ä¾¿å®éªŒèƒ½å¤ŸçœŸæ­£æœç€ç›®æ ‡å–å¾—è¿›å±•ï¼šå¦‚æœæˆ‘ä»¬å°è¯•ä¸€æ¬¡æ€§æ·»åŠ å¤šä¸ªç‰¹æ€§æˆ–å›ç­”å¤šä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯èƒ½æ— æ³•åˆ†ç¦»å„è‡ªå¯¹ç»“æœçš„å½±å“ã€‚
- ç¤ºä¾‹ç›®æ ‡åŒ…æ‹¬ï¼š
  - å°è¯•æ”¹è¿›æµç¨‹çš„æ½œåœ¨æ–¹æ³•ï¼ˆä¾‹å¦‚æ–°çš„æ­£åˆ™åŒ–å™¨ã€é¢„å¤„ç†é€‰æ‹©ç­‰ï¼‰ã€‚
  - äº†è§£ç‰¹å®šæ¨¡å‹è¶…å‚æ•°çš„å½±å“ï¼ˆä¾‹å¦‚æ¿€æ´»å‡½æ•°ï¼‰ã€‚
  - è´ªå©ªåœ°å°†éªŒè¯é”™è¯¯æœ€å°åŒ–ã€‚

### Designing the next round of experiments

**_è®¾è®¡ä¸‹ä¸€è½®å®éªŒ_**

**_æ¦‚è¦ï¼š_** _ç¡®å®šå®éªŒç›®æ ‡çš„ç§‘å­¦è¶…å‚æ•°ã€å¹²æ‰°è¶…å‚æ•°å’Œå›ºå®šè¶…å‚æ•°ã€‚åˆ›å»ºä¸€ç³»åˆ—ç ”ç©¶ï¼Œæ¯”è¾ƒç§‘å­¦è¶…å‚æ•°çš„ä¸åŒå€¼ï¼ŒåŒæ—¶ä¼˜åŒ–å¹²æ‰°è¶…å‚æ•°ã€‚é€‰æ‹©å¹²æ‰°è¶…å‚æ•°çš„æœç´¢ç©ºé—´ï¼Œä»¥å¹³è¡¡èµ„æºæˆæœ¬å’Œç§‘å­¦ä»·å€¼ã€‚_

#### Identifying scientific, nuisance, and fixed hyperparameters

<details><summary><em>[Click to expand]</em></summary>

<br>

- For a given goal, all hyperparameters will be either **scientific
  hyperparameters**, **nuisance hyperparameters**, or **fixed
  hyperparameters**.
  - Scientific hyperparameters are those whose effect on the model's
    performance we're trying to measure.
  - Nuisance hyperparameters are those that need to be optimized over in
    order to fairly compare different values of the scientific
    hyperparameters. This is similar to the statistical concept of
    [nuisance parameters](https://en.wikipedia.org/wiki/Nuisance_parameter).
  - Fixed hyperparameters will have their values fixed in the current round
    of experiments. These are hyperparameters whose values do not need to
    (or we do not want them to) change when comparing different values of
    the scientific hyperparameters.
    - By fixing certain hyperparameters for a set of experiments, we must
      accept that conclusions derived from the experiments might not be
      valid for other settings of the fixed hyperparameters. In other
      words, fixed hyperparameters create caveats for any conclusions we
      draw from the experiments.
- For example, if our goal is to "determine whether a model with more hidden
  layers will reduce validation error", then the number of hidden layers is a
  scientific hyperparameter.
  - The learning rate is a nuisance hyperparameter because we can only
    fairly compare models with different numbers of hidden layers if the
    learning rate is tuned separately for each number of layers (the optimal
    learning rate generally depends on the model architecture).
  - The activation function could be a fixed hyperparameter if we have
    determined in prior experiments that the best choice of activation
    function is not sensitive to model depth, or if we are willing to limit
    our conclusions about the number of hidden layers to only cover this
    specific choice of activation function. Alternatively, it could be a
    nuisance parameter if we are prepared to tune it separately for each
    number of hidden layers.
- Whether a particular hyperparameter is a scientific hyperparameter, nuisance
  hyperparameter, or fixed hyperparameter is not inherent to that
  hyperparameter, but changes depending on the experimental goal.
  - For example, the choice of activation function could be a scientific
    hyperparameter (is ReLU or tanh a better choice for our problem?), a
    nuisance hyperparameter (is the best 5-layer model better than the best
    6-layer model when we allow several different possible activation
    functions?), or a fixed hyperparameter (for ReLU nets, does adding batch
    normalization in a particular position help?).
- When designing a new round of experiments, we first identify the scientific
  hyperparameters for our experimental goal.
  - At this stage, we consider all other hyperparameters to be nuisance
    hyperparameters.
- Next, we convert some of the nuisance hyperparameters into fixed
  hyperparameters.
  - With limitless resources, we would leave all non-scientific
    hyperparameters as nuisance hyperparameters so that the conclusions we
    draw from our experiments are free from caveats about fixed
    hyperparameter values.
  - However, the more nuisance hyperparameters we attempt to tune, the
    greater the risk we fail to tune them sufficiently well for each setting
    of the scientific hyperparameters and end up reaching the wrong
    conclusions from our experiments.
    - As described
      [below](#striking-a-balance-between-informative-and-affordable-experiments),
      we could counter this risk by increasing the computational budget,
      but often our maximum resource budget is less than would be needed
      to tune over all non-scientific hyperparameters.
  - We choose to convert a nuisance hyperparameter into a fixed
    hyperparameter when, in our judgment, the caveats introduced by fixing
    it are less burdensome than the cost of including it as a nuisance
    hyperparameter.
    - The more a given nuisance hyperparameter interacts with the
      scientific hyperparameters, the more damaging it is to fix its
      value. For example, the best value of the weight decay strength
      typically depends on the model size, so comparing different model
      sizes assuming a single specific value of the weight decay would not
      be very insightful.
- Although the type we assign to each hyperparameter depends on the
  experimental goal, we have the following rules of thumb for certain
  categories of hyperparameters:
  - Of the various optimizer hyperparameters (e.g. the learning rate,
    momentum, learning rate schedule parameters, Adam betas etc.), at least
    some of them will be nuisance hyperparameters because they tend to
    interact the most with other changes.
    - They are rarely scientific hyperparameters because a goal like "what
      is the best learning rate for the current pipeline?" doesn't give
      much insight â€“ the best setting could easily change with the next
      pipeline change anyway.
    - Although we might fix some of them occasionally due to resource
      constraints or when we have particularly strong evidence that they
      don't interact with the scientific parameters, we should generally
      assume that optimizer hyperparameters must be tuned separately to
      make fair comparisons between different settings of the scientific
      hyperparameters, and thus shouldn't be fixed.
      - Furthermore, we have no _a priori_ reason to prefer one
        optimizer hyperparameter value over another (e.g. they don't
        usually affect the computational cost of forward passes or
        gradients in any way).
  - In contrast, the _choice_ of optimizer is typically a scientific
    hyperparameter or fixed hyperparameter.
    - It is a scientific hyperparameter if our experimental goal involves
      making fair comparisons between two or more different optimizers
      (e.g. "determine which optimizer produces the lowest validation
      error in a given number of steps").
    - Alternatively, we might make it a fixed hyperparameter for a variety
      of reasons, including (1) prior experiments make us believe that the
      best optimizer for our problem is not sensitive to current
      scientific hyperparameters; and/or (2) we prefer to compare values
      of the scientific hyperparameters using this optimizer because its
      training curves are easier to reason about; and/or (3) we prefer to
      use this optimizer because it uses less memory than the
      alternatives.
  - Hyperparameters introduced by a regularization technique are typically
    nuisance hyperparameters, but whether or not we include the
    regularization technique at all is a scientific or fixed hyperparameter.
    - For example, dropout adds code complexity, so when deciding whether
      to include it we would make "no dropout" vs "dropout" a scientific
      hyperparameter and the dropout rate a nuisance hyperparameter.
      - If we decide to add dropout to our pipeline based on this
        experiment, then the dropout rate would be a nuisance
        hyperparameter in future experiments.
  - Architectural hyperparameters are often scientific or fixed
    hyperparameters because architecture changes can affect serving and
    training costs, latency, and memory requirements.
    - For example, the number of layers is typically a scientific or fixed
      hyperparameter since it tends to have dramatic consequences for
      training speed and memory usage.
- In some cases, the sets of nuisance and fixed hyperparameters will depend on
  the values of the scientific hyperparameters.
  - For example, suppose we are trying to determine which optimizer out of
    Nesterov momentum and Adam results in the lowest validation error. The
    scientific hyperparameter is the `optimizer`, which takes values
    `{"Nesterov_momentum", "Adam"}`. The value
    `optimizer="Nesterov_momentum"` introduces the nuisance/fixed
    hyperparameters `{learning_rate, momentum}`, but the value
    `optimizer="Adam"` introduces the nuisance/fixed hyperparameters
    `{learning_rate, beta1, beta2, epsilon}`.
  - Hyperparameters that are only present for certain values of the
    scientific hyperparameters are called **conditional hyperparameters**.
  - We should not assume two conditional hyperparameters are the same just
    because they have the same name! In the above example, the conditional
    hyperparameter called `learning_rate` is a _different_ hyperparameter
    for `optimizer="Nesterov_momentum"` versus `optimizer="Adam"`. Its role
    is similar (although not identical) in the two algorithms, but the range
    of values that work well in each of the optimizers is typically
    different by several orders of magnitude.

</details>

#### Creating a set of studies

<details><summary><em>[Click to expand]</em></summary>

<br>

- Once we have identified the scientific and nuisance hyperparameters, we
  design a "study" or sequence of studies to make progress towards the
  experimental goal.
  - A study specifies a set of hyperparameter configurations to be run for
    subsequent analysis. Each configuration is called a "trial".
  - Creating a study typically involves choosing the hyperparameters that
    will vary across trials, choosing what values those hyperparameters can
    take on (the "search space"), choosing the number of trials, and
    choosing an automated search algorithm to sample that many trials from
    the search space. Alternatively, we could create a study by specifying
    the set of hyperparameter configurations manually.
- The purpose of the studies is to run the pipeline with different values of
  the scientific hyperparameters, while at the same time **"optimizing away"**
  (or "optimizing over") the nuisance hyperparameters so that comparisons
  between different values of the scientific hyperparameters are as fair as
  possible.
- In the simplest case, we would make a separate study for each configuration
  of the scientific parameters, where each study tunes over the nuisance
  hyperparameters.
  - For example, if our goal is to select the best optimizer out of Nesterov
    momentum and Adam, we could create one study in which
    `optimizer="Nesterov_momentum"` and the nuisance hyperparameters are
    `{learning_rate, momentum}`, and another study in which
    `optimizer="Adam"` and the nuisance hyperparameters are `{learning_rate,
beta1, beta2, epsilon}`. We would compare the two optimizers by
    selecting the best performing trial from each study.
  - We can use any gradient-free optimization algorithm, including methods
    such as Bayesian optimization or evolutionary algorithms, to optimize
    over the nuisance hyperparameters, although
    [we prefer](#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning)
    to use quasi-random search in the
    [exploration phase](#exploration-vs-exploitation) of tuning because of a
    variety of advantages it has in this setting.
    [After exploration concludes](#after-exploration-concludes), if
    state-of-the-art Bayesian optimization software is available, that is
    our preferred choice.
- In the more complicated case where we want to compare a large number of
  values of the scientific hyperparameters and it is impractical to make that
  many independent studies, we can include the scientific parameters in the
  same search space as the nuisance hyperparameters and use a search algorithm
  to sample values of _both_ the scientific and nuisance hyperparameters in a
  single study.
  - When taking this approach, conditional hyperparameters can cause
    problems since it is hard to specify a search space unless the set of
    nuisance hyperparameters is the same for all values of the scientific
    hyperparameters.
  - In this case,
    [our preference](#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning)
    for using quasi-random search over fancier black-box optimization tools
    is even stronger, since it ensures that we obtain a relatively uniform
    sampling of values of the scientific hyperparameters. Regardless of the
    search algorithm, we need to make sure somehow that it searches the
    scientific parameters uniformly.

</details>

#### Striking a balance between informative and affordable experiments

<details><summary><em>[Click to expand]</em></summary>

<br>

- When designing a study or sequence of studies, we need to allocate a limited
  budget in order to adequately achieve the following three desiderata:
  1.  Comparing enough different values of the scientific hyperparameters.
  2.  Tuning the nuisance hyperparameters over a large enough search space.
  3.  Sampling the search space of nuisance hyperparameters densely enough.
- The better we can achieve these three desiderata, the more insight we can
  extract from our experiment.
  - Comparing as many values of the scientific hyperparameters as possible
    broadens the scope of the insights we gain from the experiment.
  - Including as many nuisance hyperparameters as possible and allowing each
    nuisance hyperparameter to vary over as wide a range as possible
    increases our confidence that a "good" value of the nuisance
    hyperparameters **exists** in the search space for each configuration of
    the scientific hyperparameters.
    - Otherwise, we might make unfair comparisons between values of the
      scientific hyperparameters by not searching possible regions of the
      nuisance parameter space where better values might lie for some
      values of the scientific parameters.
  - Sampling the search space of nuisance hyperparameters as densely as
    possible increases our confidence that any good settings for the
    nuisance hyperparameters that happen to exist in our search space will
    be found by the search procedure.
    - Otherwise, we might make unfair comparisons between values of the
      scientific parameters due to some values getting luckier with the
      sampling of the nuisance hyperparameters.
- Unfortunately, improvements in _any_ of these three dimensions require
  either increasing the number of trials, and therefore increasing the
  resource cost, or finding a way to save resources in one of the other
  dimensions.
  - Every problem has its own idiosyncrasies and computational constraints,
    so how to allocate resources across these three desiderata requires some
    level of domain knowledge.
  - After running a study, we always try to get a sense of whether the study
    tuned the nuisance hyperparameters well enough (i.e. searched a large
    enough space extensively enough) to fairly compare the scientific
    hyperparameters (as described in greater detail
    [below](#extracting-insight-from-experimental-results)).

</details>

### Extracting insight from experimental results

**_Summary:_** _In addition to trying to achieve the original scientific goal of
each group of experiments, go through a checklist of additional questions and,
if issues are discovered, revise the experiments and rerun them._

- Ultimately, each group of experiments has a specific goal and we want to
  evaluate the evidence the experiments provide toward that goal.
  - However, if we ask the right questions, we will often find issues that
    need to be corrected before a given set of experiments can make much
    progress towards their original goal.
    - If we donâ€™t ask these questions, we may draw incorrect conclusions.
  - Since running experiments can be expensive, we also want to take the
    opportunity to extract other useful insights from each group of
    experiments, even if these insights are not immediately relevant to the
    current goal.
- Before analyzing a given set of experiments to make progress toward their
  original goal, we should ask ourselves the following additional questions:
  - [Is the search space large enough?](#identifying-bad-search-space-boundaries)
    - If the optimal point from a study is near the boundary of the search
      space in one or more dimensions, the search is probably not wide
      enough. In this case, we should run another study with an expanded
      search space.
  - [Have we sampled enough points from the search space?](#not-sampling-enough-points-in-the-search-space)
    - If not, run more points or be less ambitious in the tuning goals.
  - What fraction of the trials in each study are **infeasible** (i.e.
    trials that diverge, get really bad loss values, or fail to run at all
    because they violate some implicit constraint)?
    - When a very large fraction of points in a study are **infeasible**
      we should try to adjust the search space to avoid sampling such
      points, which sometimes requires reparameterizing the search space.
    - In some cases, a large number of infeasible points can indicate a
      bug in the training code.
  - [Does the model exhibit optimization issues?](#how-can-optimization-failures-be-debugged-and-mitigated)
  - [What can we learn from the training curves of the best trials?](#examining-the-training-curves)
    - For example, do the best trials have training curves consistent with
      problematic overfitting?
- If necessary, based on the answers to the questions above, refine the most
  recent study (or group of studies) to improve the search space and/or sample
  more trials, or take some other corrective action.
- Once we have answered the above questions, we can move on to evaluating the
  evidence the experiments provide towards our original goal (for example,
  [evaluating whether a change is useful](#detecting-whether-a-change-is-useful-with-isolation-plots)).

#### Identifying bad search space boundaries

<details><summary><em>[Click to expand]</em></summary>

<br>

- A search space is suspicious if the best point sampled from it is close to
  its boundary. We might find an even better point if we expanded the search
  range in that direction.
- To check search space boundaries, we like to plot completed trials on what
  we call **basic hyperparameter axis plots** where we plot the validation
  objective value versus one of the hyperparameters (e.g. learning rate). Each
  point on the plot corresponds to a single trial.
  - The validation objective value for each trial should usually be the best
    value it achieved over the course of training.

<p align="center" id="figure-1">
<img src="assets/good_and_bad_search_spaces.png" width="98%" alt="Example of good search space boundaries">
</p>

<p align="center"><b>Figure 1:</b> Examples of bad search space boundaries and acceptable search space boundaries.</p>

- The plots in [Figure 1](#figure-1) show the error rate (lower is better)
  against the initial learning rate.
- If the best points cluster towards the edge of a search space (in some
  dimension), then the search space boundaries might need to be expanded until
  the best observed point is no longer close to the boundary.
- Often, a study will include "infeasible" trials that diverge or get very bad
  results (marked with red Xs in the above plots).
  - If all trials are infeasible for learning rates greater than some
    threshold value, and if the best performing trials have learning rates
    at the edge of that region, the model [may suffer from stability issues
    preventing it from accessing higher learning
    rates](#how-can-optimization-failures-be-debugged-and-mitigated).

</details>

#### Not sampling enough points in the search space

<details><summary><em>[Click to expand]</em></summary>

<br>

- In general,
  [it can be very difficult to know](#how-many-trials-are-needed-to-get-good-results-with-quasi-random-search)
  if the search space has been sampled densely enough. ğŸ¤–
- Running more trials is of course better, but comes at an obvious cost.
- Since it is so hard to know when we have sampled enough, we usually sample
  what we can afford and try to calibrate our intuitive confidence from
  repeatedly looking at various hyperparameter axis plots and trying to get a
  sense of how many points are in the "good" region of the search space.

</details>

#### Examining the training curves

<details><summary><em>[Click to expand]</em></summary>

<br>

**_Summary:_** _Examining the training curves is an easy way to identify common
failure modes and can help us prioritize what actions to take next._

- Although in many cases the primary objective of our experiments only
  requires considering the validation error of each trial, we must be careful
  when reducing each trial to a single number because it can hide important
  details about whatâ€™s going on below the surface.
- For every study, we always look at the **training curves** (training error
  and validation error plotted versus training step over the duration of
  training) of at least the best few trials.
- Even if this is not necessary for addressing the primary experimental
  objective, examining the training curves is an easy way to identify common
  failure modes and can help us prioritize what actions to take next.
- When examining the training curves, we are interested in the following
  questions.
- Are any of the trials exhibiting **problematic overfitting?**
  - Problematic overfitting occurs when the validation error starts
    _increasing_ at some point during training.
  - In experimental settings where we optimize away nuisance hyperparameters
    by selecting the "best" trial for each setting of the scientific
    hyperparameters, we should check for problematic overfitting in _at
    least_ each of the best trials corresponding to the settings of the
    scientific hyperparameters that weâ€™re comparing.
    - If any of the best trials exhibits problematic overfitting, we
      usually want to re-run the experiment with additional regularization
      techniques and/or better tune the existing regularization parameters
      before comparing the values of the scientific hyperparameters.
      - This may not apply if the scientific hyperparameters include
        regularization parameters, since then it would not be surprising
        if low-strength settings of those regularization parameters
        resulted in problematic overfitting.
    - Reducing overfitting is often straightforward using common
      regularization techniques that add minimal code complexity or extra
      computation (e.g. dropout, label smoothing, weight decay), so itâ€™s
      usually no big deal to add one or more of these to the next round of
      experiments.
    - For example, if the scientific hyperparameter is "number of hidden
      layers" and the best trial that uses the largest number of hidden
      layers exhibited problematic overfitting, then we would usually
      prefer to try it again with additional regularization instead of
      immediately selecting the smaller number of hidden layers.
    - Even if none of the "best" trials are exhibiting problematic
      overfitting, there might still be a problem if it occurs in _any_ of
      the trials.
      - Selecting the best trial suppresses configurations exhibiting
        problematic overfitting and favors those that do not. In other
        words, it will favor configurations with more regularization.
      - However, anything that makes training worse can act as a
        regularizer, even if it wasn't intended that way. For example,
        choosing a smaller learning rate can regularize training by
        hobbling the optimization process, but we typically don't want
        to choose the learning rate this way.
      - So we must be aware that the "best" trial for each setting of
        the scientific hyperparameters might be selected in such a way
        that favors "bad" values of some of the scientific or nuisance
        hyperparameters.
- Is there high step-to-step variance in the training or validation error late
  in training?
  - If so, this could interfere with our ability to compare different values
    of the scientific hyperparameters (since each trial randomly ends on a
    "lucky" or "unlucky" step) and our ability to reproduce the result of
    the best trial in production (since the production model might not end
    on the same "lucky" step as in the study).
  - The most likely causes of step-to-step variance are batch variance (from
    randomly sampling examples from the training set for each batch), small
    validation sets, and using a learning rate thatâ€™s too high late in
    training.
  - Possible remedies include increasing the batch size, obtaining more
    validation data, using learning rate decay, or using Polyak averaging.
- Are the trials still improving at the end of training?
  - If so, this indicates that we are in the
    ["compute bound" regime](#determining-the-number-of-steps-for-each-training-run)
    and we may benefit from
    [increasing the number of training steps](#Deciding-how-long-to-train-when-training-is-compute-bound)
    or changing the learning rate schedule.
- Has performance on the training and validation sets saturated long before
  the final training step?
  - If so, this indicates that we are in the
    ["not compute-bound"](#determining-the-number-of-steps-for-each-training-run)
    regime and that we may be able to
    [decrease the number of training steps](#deciding-how-long-to-train-when-training-is-not-compute-bound).
- Although we cannot enumerate them all, there are many other additional
  behaviors that can become evident from examining the training curves (e.g.
  training loss _increasing_ during training usually indicates a bug in the
  training pipeline).

</details>

#### Detecting whether a change is useful with isolation plots

<details><summary><em>[Click to expand]</em></summary>

<br>

<p align="center" id="figure-2">
<img src="assets/basic_isolation_plot.png" width="55%" alt="Isolation plot that investigates the best value of weight decay for ResNet-50
trained on ImageNet.">
</p>

<p align="center"><b>Figure 2:</b> Isolation plot that investigates the best value of weight decay for ResNet-50 trained on ImageNet.</p>

- Often, the goal of a set of experiments is to compare different values of a
  scientific hyperparameter.
  - For example, we may want to determine the value of weight decay that
    results in the best validation error.
- An **isolation plot** is a special case of the basic hyperparameter axis
  plot. Each point on an isolation plot corresponds to the performance of the
  _best_ trial across some (or all) of the nuisance hyperparameters.
  - In other words, we plot the model performance after "optimizing away"
    the nuisance hyperparameters.
- An isolation plot makes it easier to perform an apples-to-apples comparison
  between different values of the scientific hyperparameter.
- For example, [Figure 2](#figure-2) reveals the value of weight decay that
  produces the best validation performance for a particular configuration of
  ResNet-50 trained on ImageNet.
  - If our goal is to determine whether to include weight decay at all, then
    we would compare the best point from this plot against the baseline of
    no weight decay. For a fair comparison, the baseline should also have
    its learning rate equally well tuned.
- When we have data generated by (quasi)random search and are considering a
  continuous hyperparameter for an isolation plot, we can approximate the
  isolation plot by bucketing the x-axis values of the basic hyperparameter
  axis plot and taking the best trial in each vertical slice defined by the
  buckets.

</details>

#### Automate generically useful plots

<details><summary><em>[Click to expand]</em></summary>

<br>

- The more effort it is to generate plots, the less likely we are to look at
  them as much as we should, so it behooves us to set up our infrastructure to
  automatically produce as many of them as possible.
- At a minimum, we automatically generate basic hyperparameter axis plots for
  all hyperparameters that we vary in an experiment.
- Additionally, we automatically produce training curves for all trials and
  make it as easy as possible to find the best few trials of each study and
  examine their training curves.
- There are many other potential plots and visualizations we can add that can
  be useful. Although the ones described above are a good starting point, to
  paraphrase Geoffrey Hinton, "Every time you plot something new, you learn
  something new."

</details>

### Determining whether to adopt a training pipeline change or hyperparameter configuration

**_Summary:_** _When deciding whether to make a change to our model or training
procedure or adopt a new hyperparameter configuration going forward, we need to
be aware of the different sources of variation in our results._

- When we are trying to improve our model, we might observe that a particular
  candidate change initially achieves a better validation error compared to
  our incumbent configuration, but find that after repeating the experiment
  there is no consistent advantage. Informally, we can group the most
  important sources of variation that might cause such an inconsistent result
  into the following broad categories:
  - **Training procedure variance**, **retrain variance**, or **trial
    variance**: the variation we see between training runs that use the same
    hyperparameters, but different random seeds.
    - For example, different random initializations, training data
      shuffles, dropout masks, patterns of data augmentation operations,
      and orderings of parallel arithmetic operations, are all potential
      sources of trial variance.
  - **Hyperparameter search variance**, or **study variance**: the variation
    in results caused by our procedure to select the hyperparameters.
    - For example, we might run the same experiment with a particular
      search space, but with two different seeds for quasi-random search
      and end up selecting different hyperparameter values.
  - **Data collection and sampling variance**: the variance from any sort of
    random split into training, validation, and test data or variance due to
    the training data generation process more generally.
- It is all well and good to make comparisons of validation error rates
  estimated on a finite validation set using fastidious statistical tests, but
  often the trial variance alone can produce statistically significant
  differences between two different trained models that use the same
  hyperparameter settings.
- We are most concerned about study variance when trying to make conclusions
  that go beyond the level of an individual point in hyperparameters space.
  - The study variance depends on the number of trials and the search space
    and we have seen cases where it is larger than the trial variance as
    well as cases where it is much smaller.
- Therefore, before adopting a candidate change, consider running the best
  trial N times to characterize the run-to-run trial variance.
  - Usually, we can get away with only recharacterizing the trial variance
    after major changes to the pipeline, but in some applications we might
    need fresher estimates.
  - In other applications, characterizing the trial variance is too costly
    to be worth it.
- At the end of the day, although we only want to adopt changes (including new
  hyperparameter configurations) that produce real improvements, demanding
  complete certainty that something helps isn't the right answer either.
- Therefore, if a new hyperparameter point (or other change) gets a better
  result than the baseline (taking into account the retrain variance of both
  the new point and the baseline as best we can), then we probably should
  adopt it as the new baseline for future comparisons.
  - However, we should only adopt changes that produce improvements that
    outweigh any complexity they add.

### After exploration concludes

**_Summary:_** _Bayesian optimization tools are a compelling option once weâ€™re
done exploring for good search spaces and have decided what hyperparameters even
should be tuned at all._

- At some point, our priorities will shift from learning more about the tuning
  problem to producing a single best configuration to launch or otherwise use.
- At this point, there should be a refined search space that comfortably
  contains the local region around the best observed trial and has been
  adequately sampled.
- Our exploration work should have revealed the most essential hyperparameters
  to tune (as well as sensible ranges for them) that we can use to construct a
  search space for a final automated tuning study using as large a tuning
  budget as possible.
- Since we no longer care about maximizing our insight into the tuning
  problem, many of
  [the advantages of quasi-random search](#why-use-quasi-random-search-instead-of-more-sophisticated-black-box-optimization-algorithms-during-the-exploration-phase-of-tuning)
  no longer apply and Bayesian optimization tools should be used to
  automatically find the best hyperparameter configuration.
  - [Open-Source Vizier](https://github.com/google/vizier) implements
    a variety of sophisticated algorithms for tuning ML models, including
    Bayesian Optimization algorithms.
  - If the search space contains a non-trivial volume of divergent points
    (points that get NaN training loss or even training loss many standard
    deviations worse than the mean), it is important to use black box
    optimization tools that properly handle trials that diverge (see
    [Bayesian Optimization with Unknown Constraints](https://arxiv.org/abs/1403.5607)
    for an excellent way to deal with this issue). [Open-Source Vizier](https://github.com/google/vizier)
    has support for divergent points by marking trials as infeasible, although it may not use our preferred approach from [Gelbart et al.](https://arxiv.org/abs/1403.5607), depending on how it is configured.
- At this point, we should also consider checking the performance on the test
  set.
  - In principle, we could even fold the validation set into the training
    set and retraining the best configuration found with Bayesian
    optimization. However, this is only appropriate if there won't be future
    launches with this specific workload (e.g. a one-time Kaggle
    competition).

## Determining the number of steps for each training run

- There are two types of workloads: those that are compute-bound and those
  that are not.
- When training is **compute-bound**, training is limited by how long we are
  willing to wait and not by how much training data we have or some other
  factor.
  - In this case, if we can somehow train longer or more efficiently, we
    should see a lower training loss and, with proper tuning, an improved
    validation loss.
  - In other words, _speeding up_ training is equivalent to _improving_
    training and the "optimal" training time is always "as long as we can
    afford."
  - That said, just because a workload is compute-limited doesn't mean
    training longer/faster is the only way to improve results.
- When training is **not compute-bound**, we can afford to train as long as we
  would like to, and, at some point, training longer doesn't help much (or
  even causes problematic overfitting).
  - In this case, we should expect to be able to train to very low training
    loss, to the point where training longer might slightly reduce the
    training loss, but will not meaningfully reduce the validation loss.
  - Particularly when training is not compute-bound, a more generous
    training time budget can make tuning easier, especially when tuning
    learning rate decay schedules, since they have a particularly strong
    interaction with the training budget.
    - In other words, very stingy training time budgets might require a
      learning rate decay schedule tuned to perfection in order to achieve
      a good error rate.
- Regardless of whether a given workload is compute-bound or not, methods that
  increase the variance of the gradients (across batches) will usually result
  in slower training progress, and thus may increase the number of training
  steps required to reach a particular validation loss. High gradient variance
  can be caused by:
  - Using a smaller batch size
  - Adding data augmentation
  - Adding some types of regularization (e.g. dropout)

### Deciding how long to train when training is _not_ compute-bound

- Our main goal is to ensure we are training long enough for the model to
  reach the best possible result, while avoiding being overly wasteful in the
  number of training steps.
- When in doubt, err on the side of training longer. Performance should never
  degrade when training longer, assuming retrospective (optimal) checkpoint
  selection is used properly and checkpoints are frequent enough.
- Never tune the `max_train_steps` number in a study. Pick a value and use it
  for all trials. From these trials, plot the training step that retrospective
  checkpoint selection finds in order to refine the choice of
  `max_train_steps`.
  - For example, if the best step is always during the first 10% of
    training, then the maximum number of steps is way too high.
  - Alternatively, if the best step is consistently in the last 25% of
    training we might benefit from training longer and re-tuning the decay
    schedule.
- The ideal number of training steps can change when the architecture or data
  changes (e.g. adding data augmentation).
- Below we describe how to pick an initial candidate value for
  `max_train_steps` based on the number of steps necessary to "perfectly fit"
  the training set using a constant learning rate.
  - Note, we are not using the phrase "perfectly fit the training set" in a
    precise or mathematically well-defined way. It is merely meant as an
    informal descriptor to indicate a very low training loss.
    - For example, when training with the log loss, absent regularization
      terms, we might see the training loss keep slowly improving until we
      reach floating point limits as the network weights grow without
      bound and the predictions of the model on the training set become
      increasingly confident. In this case, we might say the model
      "perfectly fit" the training set around the time the
      misclassification error reached zero on the training set.
  - The starting value for `max_train_steps` we find may need to be
    increased if the amount of gradient noise in the training procedure
    increases.
    - For example, if data augmentation or regularizers like dropout are
      introduced to the model.
  - It may be possible to decrease `max_train_steps` if the training process
    improves somehow.
    - For example, with a better tuned optimizer or a better tuned
      learning rate schedule.

#### Algorithm for picking an initial candidate for max_train_steps using a learning rate sweep

<details><summary><em>[Click to expand]</em></summary>

<br>

- This procedure assumes it is possible to not only "perfectly" fit the
  training set, but to do so using a constant learning rate schedule.
- If it is possible to perfectly fit the entire training set, then there must
  exist a configuration (with some value of `max_train_steps`) that perfectly
  fits the training set; find any such configuration and use its value of
  `max_train_steps` as a starting point `N`.
- Run a constant learning rate sweep (i.e. grid search the learning rate)
  without data augmentation and without regularization where each trial trains
  for `N` steps.
- The number of steps required for the fastest trial in the sweep to reach
  perfect training performance is our initial guess for `max_train_steps`.
- **NOTE:** Bad search spaces can make it possible to engage in
  self-deception.
  - For example, if all the learning rates in a study are too small, we
    might incorrectly conclude that a very large value of `max_train_steps`
    is necessary.
  - At a minimum, we should check that the optimal learning rate in the
    study is not at the boundary of the search space.

</details>

### Deciding how long to train when training is compute-bound

- In some cases, training loss keeps improving indefinitely and our patience
  and computational resources become the limiting factors.
- If training loss (or even validation loss) keeps improving indefinitely,
  should we always train as long as we can afford? Not necessarily.
  - We might be able to tune more effectively by running a larger number of
    shorter experiments and reserving the longest "production length" runs
    for the models we hope to launch.
  - As the training time for trials approaches our patience limit, tuning
    experiments become more relevant for our potential launch candidates,
    but we can complete fewer of them.
  - There are probably many questions we can answer while only training for
    ~10% of the production length, but there is always a risk that our
    conclusions at this time limit will not apply to experiments at 20% of
    the production length, let alone 100%.
- Tuning in multiple rounds with increasing, per-trial training step limits is
  a sensible approach.
  - We can do as many rounds as we want, but usually 1-3 are the most
    practical.
  - Essentially, try to obtain as much understanding of the problem as
    possible using trials with a very quick turnaround time, trading off
    tuning thoroughness with relevance to the final, longest runs.
  - Once a given per-trial time limit has generated useful insights, we can
    increase the training time and continue tuning, double-checking our
    conclusions from the shorter runs as needed.
- As a starting point, we recommend two rounds of tuning:
  - Round 1: Shorter runs to find good model and optimizer hyperparameters.
  - Round 2: Very few long runs on good hyperparameter points to get the
    final model.
- The biggest question going from `Round i` &rarr; `Round i+1` is how to
  adjust learning rate decay schedules.
  - One common pitfall when adjusting learning rate schedules between rounds
    is using all the extra training steps with too small of a learning rate.

#### Round 1

<details><summary><em>[Click to expand]</em></summary>

<br>

- Unfortunately, there is no guarantee that good hyperparameters found in
  short, incomplete training are still good choices when training length is
  significantly increased. However, for some kinds of hyperparameters, they
  are often correlated enough for Round 1 to be useful.
- What hyperparameter values found in shorter runs do we expect to transfer to
  longer training runs? For all of this, we need more research. But based on
  what we know so far, here are the authorsâ€™ suspicions in order of decreasing
  probability of transferring:
  - Very likely to transfer
    - Early training instability can be resolved in the first round of
      tuning using a smaller number of training steps. Perhaps these
      hyperparameters are the closest thing to a sure bet for transfer
      that we have.
      - Warmup length
      - Initialization
  - Likely to transfer
    - Model architecture - A dramatic win in the model architecture will
      usually transfer, but there are probably many counterexamples.
  - Might transfer
    - Optimization algorithm/optimizer hyperparameters - We think this
      would "loosely" transfer. Itâ€™s definitely weaker than the things
      above it.
    - Data augmentation
    - Regularization
      - If it isn't possible to perfectly fit the training set, the
        model might be in a regime where regularization is unlikely to
        help very much.
  - Unlikely to transfer
    - Learning rate schedule: unlikely to transfer perfectly.
      - [This paper](https://arxiv.org/abs/2203.15556) suggests that
        even decay schedule transfers, but we don't believe this is true
        in general. Example: Tuning sqrt decay on small # of training
        steps then extending to large # will result in the majority of
        training occurring at overly small steps.
        - One can likely do "good enough" with most schedules in the
          limit of extreme training budget, but noticeable performance
          improvements can likely be seen if it is tuned.
      - [Understanding Short-Horizon Bias in Stochastic
        Meta-Optimization](https://arxiv.org/abs/1803.02021) describes
        the dangers of trying to pick learning rates myopically.

</details>

#### Round 2

<details><summary><em>[Click to expand]</em></summary>

<br>

- Run the best hyperparameter configuration from Round 1.
- **(Speculation)** ğŸ¤– Use the extra steps to extend the period of training at
  a high learning rate.
  - E.g. if linear schedule then keep the length of the decay fixed from
    Round 1 and extend the period of constant lr in the beginning.
  - For cosine decay, just keep the base lr from Round 1 and extend
    `max_train_steps` as in
    [Chinchilla paper](https://arxiv.org/abs/2203.15556).
- More rounds might make sense for teams with very mature modeling and tuning
  pipelines and very long and expensive production training runs, but they
  will often be overkill.
  - We've described how to transfer from Step 1 &rarr; Step 2. If we didn't care
    about analysis time and if making efficient use of compute was the
    overriding concern, then the ideal would be to exponentially increase
    the length of training runs (and thus the end-to-end time to complete a
    study) over many different rounds of tuning.
    - At each round we systematically ensure our choices continue to hold
      up.
    - New ideas go through a pipeline that progressively derisks them
      using increasingly long-running experiments from Step i to Step i+1.

</details>

## Additional guidance for the training pipeline

### Optimizing the input pipeline

**_Summary:_** _The causes and interventions of input-bound pipelines are highly
task-dependent; use a profiler and look out for common issues._

- Use an appropriate profiler to diagnose input-bound pipelines. For example,
  [Perfetto](https://jax.readthedocs.io/en/latest/profiling.html) for JAX or
  [TensorFlow profiler](https://www.tensorflow.org/guide/profiler) for
  TensorFlow.
- Ultimately, the specific causes and interventions will be highly
  task-dependent. Broader engineering considerations (e.g. minimizing disk
  footprint) may warrant worse input pipeline performance.
- Common causes:
  - Data are not colocated with the training process, causing I/O latency
    (this might happen when reading training data over a network).
  - Expensive online data preprocessing (consider doing this once offline
    and saving).
  - Unintentional synchronization barriers that interfere with data pipeline
    prefetching. For example, when synchronizing metrics between the device
    and host in CommonLoopUtils
    ([link](https://github.com/google/CommonLoopUtils/blob/fea2518ada8814a78e1492023fd9f00edb0b0568/clu/metrics.py#L291)).
- Common tips:
  - Instrument input pipeline to prefetch examples (e.g.
    [tf.data.Dataset.prefetch](https://www.tensorflow.org/guide/data_performance#prefetching))
  - Remove unused features/metadata from each as early in the pipeline as
    possible.
  - Increase the replication of the number of jobs generating examples for
    the input pipeline. For example, by using the
    [tf.data service](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service).

### Evaluating model performance

**_Summary:_** _Run evaluation at larger batch sizes than training. Run
evaluations at regular step intervals, not regular time intervals._

#### Evaluation settings

<details><summary><em>[Click to expand]</em></summary>

<br>

- There are several settings in which we can evaluate the performance of our
  models.
  - **Online evaluation** - metrics are collected when the model is serving
    predictions in a production environment.
  - **Offline evaluation** - metrics are collected when the model is run on
    offline train/validation/test sets that are representative of the
    production environment.
  - **Periodic evaluations** - metrics are collected during model training
    that might either be a proxy for the offline evaluation, and/or on a
    subset of the data used in offline evaluation.
- Online evaluation is the gold standard, but is often impractical during the
  model development phase.
- Depending on the problem, offline evaluation can be fairly involved and
  computationally expensive.
- Periodic evaluations are the most practical and economical choice, but may
  not fully represent the production environment.
  - Our goal during periodic evaluation is to use an expedient proxy of the
    offline evaluation, without sacrificing the reliability of the signal we
    get during training.

</details>

#### Setting up periodic evaluations

<details><summary><em>[Click to expand]</em></summary>

<br>

- We run periodic evaluations during training to monitor its progress in real
  time, to
  [facilitate retrospective model checkpoint selection](#saving-checkpoints-and-retrospectively-selecting-the-best-checkpoint),
  and so that we can
  [examine the training curves at the end of training](#examining-the-training-curves).
- The simplest configuration is to perform both training and periodic
  evaluations within the same compute instance, periodically alternating
  between training and evaluation.
  - In this case, the batch size used to perform evaluations should be _at
    least_ as large as the batch size used for training because model
    activations don't need to be maintained during evaluation, lowering the
    computational requirements per example.
- Periodic evaluations should be done at regular step intervals, not time
  intervals.
  - Evaluating based on time intervals can make it harder to interpret the
    training curves, especially when training may suffer from preemptions of
    the training jobs, network latency issues, etc.
- Periodicity in valid/test metrics (when using a shuffled
  train/validation/test split) can indicate implementation bugs such as test
  data having overlap with training data, or training data not being properly
  shuffled. Evaluating at regular step intervals can make these issues easier
  to catch.
- Partial batches can occur when the evaluation sets are not divisible by the
  batch size. Ensure that the padded examples are correctly weighted to prevent
  the loss function from being biased by them. Often, these padded examples
  can be given a weight of zero.
- Save sufficient information per evaluation to support offline analysis.
  Ideally, we would save predictions on a selection of individual examples
  since they can be invaluable for debugging.
  - Generating artifacts like
    [SavedModels](https://www.tensorflow.org/guide/saved_model) make it easy
    to do ad-hoc model inspection after evaluation jobs finish.

</details>

#### Choosing a sample for periodic evaluation

<details><summary><em>[Click to expand]</em></summary>

<br>

- The periodic evaluation job might not run fast enough to compute metrics on
  the full offline evaluation set in a reasonable amount of time. This often
  necessitates sampling data for periodic evaluation.
- We consider the following factors when constructing a sampled dataset:
  - <ins>Sample size</ins>
    - Check that the performance computed on the sampled dataset used by
      the periodic job matches the performance on the whole offline
      evaluation set, i.e. there is no skew between the sampled set and
      the full dataset.
    - The dataset used for periodic evaluation should be small enough that
      itâ€™s easy to generate model predictions over its entirety, but large
      enough that improvements to the model can be accurately measured
      (i.e. not overwhelmed by label noise).
    - It should be large enough to accommodate multiple such evaluations
      across trials in sequence, and still produce accurate estimates.
      That is, to avoid adaptively "fitting" to the validation set over
      time, in a way that doesn't generalize to a held-out test set.
      However, this consideration is rarely a practical concern.
  - <ins>Imbalanced datasets</ins>
    - For imbalanced datasets, performance on rare classes of examples
      will often be noisy.
    - For datasets with a small number of examples in a class label, log
      the number of examples predicted correctly to get more insight into
      accuracy improvements (.05 sensitivity improvement sounds exciting,
      but was it just one more example correct?).

</details>

### Saving checkpoints and retrospectively selecting the best checkpoint

**_Summary:_** _Run training for a fixed number of steps and retrospectively
choose the best checkpoint from the run._

- Most deep learning frameworks support
  [model checkpointing](https://flax.readthedocs.io/en/latest/api_reference/flax.training.html).
  That is, the current state of the model is periodically preserved on disk.
  This allows the training job to be resilient to compute instance
  interruptions.
- The best checkpoint is often not the last checkpoint, particularly when the
  validation set performance does not continue to increase over time but
  rather fluctuates about a particular value.
- Set up the pipeline to keep track of the N best checkpoints seen so far
  during training. At the end of training, model selection is then a matter of
  choosing the best checkpoint seen during training. We call this
  **retrospective optimal checkpoint selection**.
- Supporting prospective early stopping is usually not necessary, since weâ€™re
  pre-specifying a trial budget and are preserving the N best checkpoints seen
  so far.

### Setting up experiment tracking

**_Summary:_** _When tracking different experiments, make sure to note a number
of essentials like the best performance of a checkpoint in the study, and a
short description of the study._

- We've found that keeping track of experiment results in a spreadsheet has
  been helpful for the sorts of modeling problems we've worked on. It often
  has the following columns:
  - Study name
  - A link to wherever the config for the study is stored.
  - Notes or a short description of the study.
  - Number of trials run
  - Performance on the validation set of the best checkpoint in the study.
  - Specific reproduction commands or notes on what unsubmitted changes were
    necessary to launch training.
- Find a tracking system that captures at least the information listed above
  and is convenient for the people doing it. Untracked experiments might as
  well not exist.

### Batch normalization implementation details

**_Summary:_** _Nowadays batch norm can often be replaced with LayerNorm, but in
cases where it cannot, there are tricky details when changing the batch size or
number of hosts._

- Batch norm normalizes activations using their mean and variance over the
  current batch, but in the multi-device setting these statistics are
  different on each device unless explicitly synchronized.
- Anecdotal reports (mostly on ImageNet) say calculating these normalizing
  statistics using only ~64 examples actually works better in practice (see
  Ghost Batch Norm from [this paper](https://arxiv.org/abs/1705.08741)).
- Decoupling the total batch size and the number of examples used to calculate
  batch norm statistics is particularly useful for batch size comparisons.
- Ghost batch norm implementations do not always correctly handle the case
  where the per-device batch size > virtual batch size. In this case we'd
  actually need to subsample the batch on each device in order to get the
  proper number of batch norm statistic examples.
- Exponential moving averages used in test mode batch norm are just a linear
  combination of training statistics, so these EMAs only need to be
  synchronized before saving them in checkpoints. However, some common
  implementations of batch norm do not synchronize these EMAs and only save
  the EMA from the first device.

### Considerations for multi-host pipelines

**_Summary:_** _for logging, evals, RNGs, checkpointing, and data sharding,
multi-host training can make it very easy to introduce bugs!_

- Ensure the pipeline is only logging and checkpointing on one host.
- Make sure before evaluation or checkpointing is run, the batch norm
  statistics are synchronized across hosts.
- It is critical to have RNG seeds that are the same across hosts (for model
  initialization), and seeds that are different across hosts (for data
  shuffling/preprocessing), so make sure to mark them appropriately.
- Sharding data files across hosts is usually recommended for improved
  performance.

## FAQs

### What is the best learning rate decay schedule family?

<details><summary><em>[Click to expand]</em></summary>

<br>

- Itâ€™s an open problem. Itâ€™s not clear how to construct a set of rigorous
  experiments to confidently answer what the "best" LR decay schedule is.
- Although we don't know the best schedule family, we're confident that itâ€™s
  important to have some (non-constant) schedule and that tuning it matters.
- Different learning rates work best at different times during the
  optimization process. Having some sort of schedule makes it more likely for
  the model to hit a good learning rate.

</details>

### Which learning rate decay should I use as a default?

<details><summary><em>[Click to expand]</em></summary>
<br>

- Our preference is either linear decay or cosine decay, and a bunch of other
  schedule families are probably good too.

</details>

### Why do some papers have complicated learning rate schedules?

<details><summary><em>[Click to expand]</em></summary>
<br>

- Itâ€™s not uncommon to see papers with complicated piecewise learning rate
  (LR) decay schedules.
- Readers often wonder how the authors arrived at such a complicated schedule.
- Many complicated LR decay schedules are the result of tuning the schedule as
  a function of the validation set performance in an ad hoc way:
  1.  Start a single training run with some simple LR decay (or a constant
      learning rate).
  2.  Keep training running until the performance seems to stagnate. If this
      happens, pause training. Resume it with a perhaps steeper LR decay
      schedule (or smaller constant learning rate) from this point. Repeat
      this process until the conference/launch deadline.
- Blithely copying the resulting _schedule_ is generally not a good idea since
  the best particular schedule will be sensitive to a host of other
  hyperparameter choices.
  - Better to copy the _algorithm_ that produced the schedule, although this
    is rarely possible when arbitrary human judgment produced the schedule.
- This type of validation-error-sensitive schedule is fine to use if it can be
  fully automated, but human-in-the-loop schedules that are a function of
  validation error are brittle and not easily reproducible, so we recommend
  avoiding them.
  - Before publishing results that used such a schedule, please try to make
    it fully reproducible.

</details>

### How should Adamâ€™s hyperparameters be tuned?

<details><summary><em>[Click to expand]</em></summary>
<br>

- As discussed above, making general statements about search spaces and how
  many points one should sample from the search space is very difficult. Note
  that not all the hyperparameters in Adam are equally important. The
  following rules of thumb correspond to different "budgets" for the number of
  trials in a study.
  - If < 10 trials in a study, only tune the (base) learning rate.
  - If 10-25 trials, tune learning rate and $\beta_1$.
  - If 25+ trials, tune the learning rate, $\beta_1$ and $\epsilon$.
  - If one can run substantially more than 25 trials, additionally tune
    $\beta_2$.

</details>

### Why use quasi-random search instead of more sophisticated black box optimization algorithms during the exploration phase of tuning?

<details><summary><em>[Click to expand]</em></summary>

- Quasi-random search (based on
  [low-discrepancy sequences](https://en.wikipedia.org/wiki/Low-discrepancy_sequence))
  is our preference over fancier black box optimization tools when used as
  part of an iterative tuning process intended to maximize insight into the
  tuning problem (what we refer to as the "exploration phase"). Bayesian
  optimization and similar tools are more appropriate for the exploitation
  phase.
- Quasi-random search based on randomly shifted low-discrepancy sequences can
  be thought of as "jittered, shuffled grid search", since it uniformly, but
  randomly, explores a given search space and spreads out the search points
  more than random search.
- The advantages of quasi-random search over more sophisticated black box
  optimization tools (e.g. Bayesian optimization, evolutionary algorithms)
  include:
  1.  Sampling the search space non-adaptively makes it possible to change the
      tuning objective in post hoc analysis without rerunning experiments.
      - For example, we usually want to find the best trial in terms of
        validation error achieved at any point in training. But the
        non-adaptive nature of quasi-random search makes it possible to find
        the best trial based on final validation error, training error, or
        some alternative evaluation metric without rerunning any
        experiments.
  2.  Quasi-random search behaves in a consistent and statistically
      reproducible way.
      - It should be possible to reproduce a study from six months ago even
        if the implementation of the search algorithm changes, as long as it
        maintains the same uniformity properties. If using sophisticated
        Bayesian optimization software, the implementation might change in
        an important way between versions, making it much harder to
        reproduce an old search. It isnâ€™t always possible to roll back to an
        old implementation (e.g. if the optimization tool is run as a
        service).
  3.  Its uniform exploration of the search space makes it easier to reason
      about the results and what they might suggest about the search space.
      - For example, if the best point in the traversal of quasi-random
        search is at the boundary of the search space, this is a good (but
        not foolproof) signal that the search space bounds should be
        changed. [This section](#identifying-bad-search-space-boundaries)
        goes into more depth. However, an adaptive black box optimization
        algorithm might have neglected the middle of the search space
        because of some unlucky early trials even if it happens to contain
        equally good points, since it is this exact sort of non-uniformity
        that a good optimization algorithm needs to employ to speed up the
        search.
  4.  Running different numbers of trials in parallel versus sequentially will
      not produce statistically different results when using quasi-random
      search (or other non-adaptive search algorithms), unlike with adaptive
      algorithms.
  5.  More sophisticated search algorithms may not always handle infeasible
      points correctly, especially if they aren't designed with neural network
      hyperparameter tuning in mind.
  6.  Quasi-random search is simple and works especially well when many tuning
      trials will be running in parallel.
      - Anecdotally[^3], it is very hard for an adaptive algorithm to beat a
        quasi-random search that has 2X its budget, especially when many
        trials need to be run in parallel (and thus there are very few
        chances to make use of previous trial results when launching new
        trials).
      - Without expertise in Bayesian optimization and other advanced black
        box optimization methods, we might not achieve the benefits they
        are, in principle, capable of providing. It is hard to benchmark
        advanced black box optimization algorithms in realistic deep
        learning tuning conditions. They are a very active area of current
        research, and the more sophisticated algorithms come with their own
        pitfalls for inexperienced users. Experts in these methods are able
        to get good results, but in high-parallelism conditions the search
        space and budget tend to matter a lot more.
- That said, if our computational resources only allow a small number of
  trials to run in parallel and we can afford to run many trials in sequence,
  Bayesian optimization becomes much more attractive despite making our tuning
  results harder to interpret.

[^3]:
    Ben Recht and Kevin Jamieson
    [pointed out](http://www.argmin.net/2016/06/20/hypertuning/) how strong
    2X-budget random search is as a baseline (the
    [Hyperband paper](https://jmlr.org/papers/volume18/16-558/16-558.pdf)
    makes similar arguments), but it is certainly possible to find search
    spaces and problems where state-of-the-art Bayesian optimization
    techniques crush random search that has 2X the budget. However, in our
    experience beating 2X-budget random search gets much harder in the
    high-parallelism regime since Bayesian optimization has no opportunity to
    observe the results of previous trials.

</details>

### Where can I find an implementation of quasi-random search?

<details><summary><em>[Click to expand]</em></summary>
<br>

- [Open-Source Vizier](https://github.com/google/vizier) has an [implementation
  of quasi-ranom search](https://github.com/google/vizier/blob/main/vizier/_src/algorithms/designers/quasi_random.py). Set `algorithm="QUASI_RANDOM_SEARCH"` in [this usage example](https://oss-vizier.readthedocs.io/en/latest/guides/user/running_vizier.html).
- An alternative implementation exists
  [here](https://github.com/mlcommons/algorithmic-efficiency/blob/main/algorithmic_efficiency/halton.py).
- Both implementations above generate a Halton sequence for a given search space (intended to
  implement a shifted, scrambled Halton sequence as recommended in
  https://arxiv.org/abs/1706.03200).
- If a quasi-random search algorithm based on a low-discrepancy sequence is
  not available, it is possible to substitute pseudo random uniform search
  instead, although this is likely to be slightly less efficient.
  - In 1-2 dimensions, grid search is also acceptable, although not in
    higher dimensions (see
    [Bergstra & Bengio, 2012](https://www.jmlr.org/papers/v13/bergstra12a.html)).

</details>

### How many trials are needed to get good results with quasi-random search?

<details><summary><em>[Click to expand]</em></summary>
<br>

<p align="center">
<img src="assets/validation_error_vs_num_trials.png" width="55%" alt="A box plot showing the importance of sampling enough">
</p>

<p align="center"><b>Figure 3:</b> A ResNet-50 was tuned on ImageNet with 100
trials. Via bootstrapping, different amounts of tuning budget were simulated.
Box plots of the best performances for each trial budget are plotted above.

- There is no way to answer this question in general, but we can look at
  specific examples.
- As the Figure 3 shows, the number of trials in a study can have a
  substantial impact on the results.
  - Notice how large the interquartile ranges are when 6 trials were
    sampled, versus when 20 trials were sampled.
  - Even with 20 trials, it is likely that the difference between especially
    lucky and unlucky studies will be larger than the typical variation
    between re-trains of this model on different random seeds, with fixed
    hyperparameters, which for this workload might be around +/- 0.1% on a
    validation error rate of \~23%.

</details>

### How can optimization failures be debugged and mitigated?

<details><summary><em>[Click to expand]</em></summary>
<br>

**_Summary:_** _If the model is experiencing optimization difficulties, itâ€™s
important to fix them before trying other things. Diagnosing and correcting
training failures is an active area of research._

<p align="center">
<img src="assets/stride_instability.png" width="80%" alt="Changing the strides in a single residual block in a WideResnet results in training instability.">
</p>

<p align="center"><b>Figure 4:</b> Changing the strides in a single residual block (2x2 -> 1x1) in a WideResnet results in training instability. This does not degrade performance at low learning rates, but high learning rates no longer train well due to the instability. Applying 1000 steps of learning rate warmup resolves this particular instance of instability, allowing stable training at max learning rate of .1.</p>

#### Identifying unstable workloads

- Any workload will become unstable if the learning rate is too large.
  Instability is only an issue when it forces us to use a learning rate thatâ€™s
  too small.
- There are at least two types of training instability worth distinguishing:
  1.  Instability at initialization/early in training.
  2.  Sudden instability in the middle of training.
- We can take a systematic approach to identifying stability issues in our
  workload.
  1.  Do a learning rate sweep and find the best learning rate lr\*.
  2.  Plot training loss curves for learning rates just above lr\*.
  3.  If the learning rates > lr\* show loss instability (loss goes up not down
      during periods of training), then it is likely that fixing the
      instability will result in better training.
- Log the L2 norm of the full loss gradient during training, outlier values
  can result in spurious instability in the middle of training. This can
  inform how to pick gradient/update clipping.

**NOTE:** Some models show very early instability followed by a recovery that
results in slow but stable training. **Common evaluation schedules can miss
these issues by not evaluating frequently enough!**

To check for this, we can train for an abbreviated run of just \~500 steps using
`lr = 2 * current best`, but evaluate every step.

<p align="center">
<img src="assets/more_frequent_evals.png" width="80%" alt="Illustration of the value of more frequent evaluations at the start of
training.">
</p>

<p align="center"><b>Figure 5:</b> Illustration of the value of more frequent evaluations at the start of training. Useful if thereâ€™s a suspicion that the model suffers from early training instability.</p>

#### Potential fixes for common instability patterns

- Apply learning rate warmup
  - Best for early training instability.
- Apply gradient clipping
  - Good for both early and mid training instability, may fix some bad inits
    that warmup cannot.
- Try a new optimizer
  - Sometimes Adam can handle instabilities that Momentum canâ€™t. This is an
    active area of research.
- We can ensure that weâ€™re using best practices/initializations for our model
  architecture (examples below).
  - Add residual connections and normalization if the model doesn't contain
    it already.
- Normalization should be the last operation before the residual. E.g. x +
  Norm(f(x)).
- Norm(x + f(x)) known to cause issues.
- Try initializing residual branches to 0 (e.g.
  [ReZero init](https://arxiv.org/abs/2003.04887)).
- Lower the learning rate
  - This is a last resort.

#### Learning rate warmup

<p align="center">
<img src="assets/instability_during_warmup.png" width="80%" alt="An example of instability during a warmup period (note the horizontal axis log
scale).">
</p>

<p align="center"><b>Figure 6:</b> An example of instability during a warmup period (note the horizontal axis log scale). 40k steps of warmup was needed for successful training in this case.</p>

##### When to apply learning rate warmup

<p align="center">
<img src="assets/axis_model_with_instability.png" width="49%" alt="Axis plot for model with instability">
</p>

<p align="center"><b>Figure 7a:</b> An example of a hyperparameter axis plot for a model exhibiting training instability. The best learning rate is at the edge of what is feasible. An "infeasible" trial is defined as one that either produces NaNs or uncharacteristically high values of the loss.</p>

<p align="center">
<img src="assets/loss_model_with_instability.png" width="49%" alt="Loss curve for model with instability">
</p>

<p align="center"><b>Figure 7b:</b> The training loss of a model trained with a learning rate where we see instability.</p>

- Figure 7a shows a hyperparameter axis plot that indicates a model
  experiencing optimization instabilities, because the best learning rate is
  right at the edge of instability.
- Figure 7b shows how this can be double-checked by examining the training
  loss of a model trained with a learning rate either 5x or 10x larger than
  this peak. If that plot shows a sudden rise in the loss after a steady
  decline (e.g. at step \~10k in the figure above), then the model likely
  suffers from optimization instability.

##### How to apply learning rate warmup

<p align="center">
<img src="assets/beneficial_effect_warmup.png" width="80%" alt="Beneficial effect of warmup on training instabilities">
</p>

<p align="center"><b>Figure 8:</b> Beneficial effect of learning rate warmup on addressing training instabilities.</p>

- Using the section immediately above, we assume that the practitioner has
  already identified the learning rate at which the model becomes unstable.
  This is the `unstable_base_learning_rate`.
- Warmup involves prepending a learning rate schedule that ramps up the
  learning rate from 0 to some stable `base_learning_rate`, that is at least
  one order of magnitude larger than `unstable_base_learning_rate`. The
  default would be to try a `base_learning_rate` thatâ€™s 10x
  `unstable_base_learning_rate`. Although note that itâ€™d be possible to run
  this entire procedure again for something like 100x
  `unstable_base_learning_rate`. The specific schedule is:
  - Ramp up from 0 to `base_learning_rate` over `warmup_steps`.
  - Train at a constant rate for `post_warmup_steps`.
- Our goal is to find the shortest number of `warmup_steps` that allows us to
  access peak learning rates that are much higher than
  `unstable_base_learning_rate`.
- So for each `base_learning_rate`, we need to tune `warmup_steps` and
  `post_warmup_steps`. Itâ€™s usually fine to set `post_warmup_steps` to be
  `2*warmup_steps`.
- Warmup can be tuned independently of an existing decay schedule.
  `warmup_steps` should be swept at a few different orders of magnitude. For
  example, an example study could try [10, 10<sup>3</sup>, 10<sup>4</sup>,
  10<sup>5</sup>]. The largest feasible point shouldn't be more than 10% of
  `max_train_steps`.
- Once a `warmup_steps` that doesn't blow up training at `base_learning_rate`
  has been established, it should be applied to the baseline model.
  Essentially, we prepend this schedule onto the existing schedule, and use
  the optimal checkpoint selection discussed above to compare this experiment
  to the baseline. For example, if we originally had 10,000 `max_train_steps`
  and did `warmup_steps` for 1000 steps, the new training procedure should run
  for 11,000 steps total.
- If long `warmup_steps` are required for stable training (>5% of
  `max_train_steps`), `max_train_steps` may need to be increased to account
  for this.
- There isn't really a "typical" value across the full range of workloads.
  Some models only need 100 steps, while others (particularly transformers)
  may need 40k+.

#### Gradient clipping

<p align="center">
<img src="assets/gradient_clipping.png" width="80%" alt="Gradient clipping on early training instabilities">
</p>

<p align="center"><b>Figure 9:</b> Illustration of gradient clipping correcting early training instability.</p>

- Gradient clipping is most useful when large or outlier gradient issues
  occur.
- Clipping can fix either early training instability (large gradient norm
  early), or mid training instabilities (sudden gradient spikes mid training).
- Sometimes longer warmup periods can correct instabilities that clipping does
  not: see [this section above](#How-to-apply-learning-rate-warmup).
  - ğŸ¤– What about clipping during warmup?
- The ideal clip thresholds are just above the "typical" gradient norm.
- Hereâ€™s an example of how gradient clipping could be done:
  - If the norm of the gradient $\left | g \right |$ is greater than the
    gradient clipping threshold $\lambda$, then do ${g}'= \lambda \times \frac{g}{\left | g \right |}$ where ${g}'$ is the new gradient.
- Log the unclipped gradient norm during training. By default, generate:
  - A plot of gradient norm vs step
  - A histogram of gradient norms aggregated over all steps
- Choose a gradient clipping threshold based on the 90th percentile of
  gradient norms.
  - The threshold will be workload dependent, but 90% is a good starting
    point. If it doesn't work, this threshold can be tuned.
  - ğŸ¤– What about some sort of adaptive strategy?
- If we try gradient clipping and the instability issues remain, we can try it
  harder (i.e. make the threshold smaller).
- Extremely aggressive gradient clipping is in essence a strange way of
  reducing the learning rate. If we find ourselves using extremely aggressive
  clipping, we probably should just cut the learning rate instead.
- We would usually consider having >50% of the updates getting clipped somehow
  as "extremely aggressive".
- If we need to do extremely aggressive gradient clipping to deal with our
  instability issues, then we might as well reduce the learning rate.

</details>

### Why do you call the learning rate and other optimization parameters hyperparameters? They are not parameters of any prior distribution.

<details><summary><em>[Click to expand]</em></summary>
<br>

- It is true that the term "hyperparameter" has a precise
  [meaning](https://en.wikipedia.org/wiki/Hyperparameter) in Bayesian machine
  learning and referring to the learning rate and most of the other parameters
  we tune in deep learning as "hyperparameters" is an abuse of terminology.
- We would prefer to use the term "metaparameter" for learning rates,
  architectural parameters, and all the other things we tune in deep learning,
  since it avoids the potential for confusion that comes from misusing the
  word "hyperparameter" (confusion that is especially likely when discussing
  Bayesian optimization where the probabilistic response surface models have
  their own true hyperparameters).
- Unfortunately, although potentially confusing, the term hyperparameter has become
  extremely common in the deep learning community.
- Therefore, for a document, such as this one, intended for a wide audience
  that includes many people who are unlikely to be aware of this technicality,
  we made the choice to contribute to one source of confusion in the
  field in hopes of avoiding another.
- That said, we might make a different choice when publishing a research
  paper, and we would encourage others to use "metaparameter" instead in most
  contexts.

</details>

### Why shouldn't the batch size be tuned to directly improve validation set performance?

<details><summary><em>[Click to expand]</em></summary>
<br>

- Changing the batch size _without changing any other details of the training pipeline_ will often affect the validation set performance.
- However, the difference in validation set performance between two batch sizes typically goes away if the training pipeline is optimized independently for each batch size.
- The hyperparameters that interact most strongly with the batch size, and therefore are most important to tune separately for each batch size, are the optimizer hyperparameters (e.g. learning rate, momentum) and the regularization hyperparameters.
  - Smaller batch sizes introduce more noise into the training algorithm due to sample variance, and this noise can have a regularizing effect. Thus, larger batch sizes can be more prone to overfitting and may require stronger regularization and/or additional regularization techniques.
- In addition, [the number of training steps may need to be adjusted](#choosing-the-batch-size-to-minimize-training-time) when changing the batch size.
- Once all these effects are taken into account, there is currently no convincing evidence that the batch size affects the maximum achievable validation performance (see [Shallue et al. 2018](https://arxiv.org/abs/1811.03600)).

</details>

### What are the update rules for all the popular optimization algorithms?

<details><summary><em>[Click to expand]</em></summary>

<br>

#### Stochastic gradient descent (SGD)

$$\theta_{t+1} = \theta_{t} - \eta_t \nabla \mathcal{l}(\theta_t)$$

#### Momentum

$$v_0 = 0$$

$$v_{t+1} = \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$

$$\theta_{t+1} = \theta_{t} - \eta_t v_{t+1}$$

#### Nesterov

$$v_0 = 0$$

$$v_{t+1} = \gamma v_{t} + \nabla \mathcal{l}(\theta_t)$$

$$\theta_{t+1} = \theta_{t} - \eta_t( \gamma v_{t+1} + \nabla \mathcal{l}(\theta_{t})$$

#### RMSProp

$$v_0 = 1 \text{,} m_0 = 0$$

$$v_{t+1} = \rho v_{t} + (1 - \rho) \nabla \mathcal{l}(\theta_t)^2$$

$$m_{t+1} = \gamma m_{t} + \frac{\eta_t}{\sqrt{v_{t+1} + \epsilon}}\nabla \mathcal{l}(\theta_t)$$

$$\theta_{t+1} = \theta_{t} - m_{t+1}$$

#### ADAM

$$m_0 = 0 \text{,} v_0 = 0$$

$$m_{t+1} = \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$

$$v_{t+1} = \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l}(\theta_t)^2$$

$$b_{t+1} = \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$

$$\theta_{t+1} = \theta_{t} - \alpha_t \frac{m_{t+1}}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$

#### NADAM

$$m_0 = 0 \text{,} v_0 = 0$$

$$m_{t+1} = \beta_1 m_{t} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)$$

$$v_{t+1} = \beta_2 v_{t} + (1 - \beta_2) \nabla \mathcal{l} (\theta_t)^2$$

$$b_{t+1} = \frac{\sqrt{1 - \beta_2^{t+1}}}{1 - \beta_1^{t+1}}$$

$$\theta_{t+1} = \theta_{t} - \alpha_t \frac{\beta_1 m_{t+1} + (1 - \beta_1) \nabla \mathcal{l} (\theta_t)}{\sqrt{v_{t+1}} + \epsilon} b_{t+1}$$

</details>

## Acknowledgments

- We owe a debt of gratitude to Max Bileschi, Roy Frostig, Zelda Mariet, Stan
  Bileschi, Mohammad Norouzi, Chris DuBois and Charles Sutton for reading the
  manuscript and providing valuable feedback.
- We reused some experimental data for several plots that were originally
  produced by Naman Agarwal for other joint research.
- We would like to thank Will Chen for invaluable advice on the presentation of the document.
- We would also like to thank Rohan Anil for useful discussions.

## Citing

```
@misc{tuningplaybookgithub,
  author = {Varun Godbole and George E. Dahl and Justin Gilmer and Christopher J. Shallue and Zachary Nado},
  title = {Deep Learning Tuning Playbook},
  url = {http://github.com/google-research/tuning_playbook},
  year = {2023},
  note = {Version 1.0}
}
```

## Contributing

- This is not an officially supported Google product.

- We'd love to hear your feedback!

  - If you like the playbook, please [leave a star](https://docs.github.com/en/get-started/exploring-projects-on-github/saving-repositories-with-stars#starring-a-repository)! Or email
    deep-learning-tuning-playbook \[at\] googlegroups.com. Testimonials help
    us justify creating more resources like this.
  - If anything seems incorrect, please file an issue to start a discussion.
    For questions or other messages where an issue isn't appropriate, please
    open a new discussion topic on GitHub.

- As discussed in the preamble, this is a living document. We anticipate
  making periodic improvements, both small and large. If youâ€™d like to be
  notified, please watch our repository (see [instructions](https://docs.github.com/en/account-and-profile/managing-subscriptions-and-notifications-on-github/setting-up-notifications/configuring-notifications#configuring-your-watch-settings-for-an-individual-repository)).

- Please don't file a pull request without first coordinating with the authors
  via the issue tracking system.

### Contributor License Agreement

Contributions to this project must be accompanied by a Contributor License
Agreement (CLA). You (or your employer) retain the copyright to your
contribution; this simply gives us permission to use and redistribute your
contributions as part of the project. Head over to
<https://cla.developers.google.com/> to see your current agreements on file or
to sign a new one.

You generally only need to submit a CLA once, so if you've already submitted one
(even if it was for a different project), you probably don't need to do it
again.

### Code Reviews

All submissions, including submissions by project members, require review. We
use GitHub pull requests for this purpose. Consult
[GitHub Help](https://help.github.com/articles/about-pull-requests/) for more
information on using pull requests.

### Community Guidelines

This project follows
[Google's Open Source Community Guidelines](https://opensource.google/conduct/).
